{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uo0JP7a5uFp7"
   },
   "source": [
    "# **Notebook to uncover the key parts of ASR**\n",
    "\n",
    "This tutorial will walk you through all the modules needed to implement an offline **end-to-end attention-based speech recognizer** on Speechbrain.\n",
    "\n",
    "For simplicity, we are not training any model, but rather using the models (AM/LM/Tokenizer) available from huggingface hub. The models are trained in an open-source dataset called [librispeech](https://www.openslr.org/12/) with 960 hours of train data.\n",
    "\n",
    "In this tutorial, we will refer to the code in ```NLP_Summer_School-2021_Speech_Tutorial/ASR/LibriSpeech/{ASR,LM,Tokenizer}```. \n",
    "You could follow up a more detailed Colab Notebook about training ASR from Scratch: [Colab Notebook - Train from Scratch](https://colab.research.google.com/drive/1aFgzrUv3udM_gNJNUoLaHIm78QHtxdIz?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eWpl9xgAIXKE"
   },
   "source": [
    "## **Which modules are we covering today?**\n",
    "\n",
    "In order to train the LM and AM, you would need to prepare LibriSpeech folder + download all the required material. Training could take days in cluster with several GPUs. \n",
    "\n",
    "0. **Data preparation**.\n",
    "For this tutorial we won't need any data preparation step, because we will take adavantage of the Speechbrain class `EncoderDecoderASR`; which could apply inference in one simple `wav` file.  \n",
    "\n",
    "1. **Tokenizer**.\n",
    "The tokenizer decides which basic units are allocated during ASR training/infernce (e.g, characters, phonemes, sub-words, words).\n",
    "\n",
    "```\n",
    "cd NLP_Summer_School-2021_Speech_Tutorial/ASR/LibriSpeech/Tokenizer\n",
    "python train.py tokenizer.yaml\n",
    "```\n",
    "\n",
    "2. **The language model**.\n",
    "After that, the language model could be trained (we just used during inference). In this example, however, we don't train it (rather download a pre-trained version)\n",
    "\n",
    "We need an additional Python (Huggingface) library: `datasets`\n",
    "```\n",
    "pip install datasets\n",
    "cd NLP_Summer_School-2021_Speech_Tutorial/ASR/LibriSpeech/LM\n",
    "python train.py hparams/transformer.yaml\n",
    "```\n",
    "\n",
    "3. **Automatic speech recognizer - Speech-to-text system**.\n",
    "At this point, we are ready to train our speech recognizer. In this tutorial, we will use the CRDNN model with an autoregressive GRU decoder. An attention mechanism is employed between encoding and decoder. The final sequence of words is retrieved with beamsearch coupled with the Transformer LM fetched in the previous stes:\n",
    "```\n",
    "cd NLP_Summer_School-2021_Speech_Tutorial/ASR/LibriSpeech/ASR/transformer\n",
    "python train.py hparams/transformer.yaml\n",
    "```\n",
    "\n",
    "4. **Use the speech recognizer (inference)**:\n",
    "After training, we can use the speech recognizer for inference. We will use the `EncoderDecoderASR` class available in SpeechBrain to make inference.\n",
    "\n",
    "(Most of this tutorial is based on the [ASRfromScratch](https://colab.research.google.com/drive/1aFgzrUv3udM_gNJNUoLaHIm78QHtxdIz?usp=sharing) Google Colab! Thanks!)\n",
    "\n",
    "We will go through each of these "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDgNu_b8k6qD"
   },
   "source": [
    "## **Step 0: Prepare your data** \n",
    "\n",
    "**!! You don't need to do anything here for the NLP summer school speech Tutorial. In case you'd like to continue training your own ASR engine, you could follow the notebooks' links at the end of this one.**\n",
    "\n",
    "The goal of data preparation is to create the data manifest files. \n",
    "These files tell SpeechBrain where to find the audio data and their corresponding transcriptions. They are text files written in the popular CSV and JSON formats.\n",
    "\n",
    "### **Data manifest files**\n",
    "Let's take a look into how a data manifest file in JSON format looks like:\n",
    "\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"1867-154075-0032\": {\n",
    "    \"wav\": \"{data_root}/LibriSpeech/train-clean-5/1867/154075/1867-154075-0032.flac\",\n",
    "    \"length\": 16.09,\n",
    "    \"words\": \"AND HE BRUSHED A HAND ACROSS HIS FOREHEAD AND WAS INSTANTLY HIMSELF CALM AND COOL VERY WELL THEN IT SEEMS I'VE MADE AN ASS OF MYSELF BUT I'LL TRY TO MAKE UP FOR IT NOW WHAT ABOUT CAROLINE\"\n",
    "  },\n",
    "  \"1867-154075-0001\": {\n",
    "    \"wav\": \"{data_root}/LibriSpeech/train-clean-5/1867/154075/1867-154075-0001.flac\",\n",
    "    \"length\": 14.9,\n",
    "    \"words\": \"THAT DROPPED HIM INTO THE COAL BIN DID HE GET COAL DUST ON HIS SHOES RIGHT AND HE DIDN'T HAVE SENSE ENOUGH TO WIPE IT OFF AN AMATEUR A RANK AMATEUR I TOLD YOU SAID THE MAN OF THE SNEER WITH SATISFACTION\"\n",
    "  },\n",
    "}\n",
    "```\n",
    "As you can see, we have a hierarchical structure in which: \n",
    "\n",
    "- Key: **unique identifier** of the spoken sentence,\n",
    "- First item: **path of the speech recording**,\n",
    "- Second item: **length**, if we have a segments file we might need to change this, \n",
    "- Third item: **sequence of words** for the given train/test sample.\n",
    "\n",
    "### **Preparation Script**\n",
    "Every dataset is formatted in a different way. The script that parses your own dataset and creates the JSON or the CSV files is something that you are supposed to write. Most of the time, this is very straightforward. \n",
    "\n",
    "For the mini-librispeech dataset, for instance, we wrote this simple data preparation script called [mini_librispeech_prepare.py](https://github.com/speechbrain/speechbrain/blob/develop/templates/speech_recognition/mini_librispeech_prepare.py).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9To_-2fej2SA"
   },
   "source": [
    "## **Step 1: Tokenizer** \n",
    "An important decision to make when designing a speech recognizer concerns the basic tokens that our system has to predict (e.g, characters, phonemes, sub-words, words).\n",
    "\n",
    "### **Using characters as tokens**\n",
    "One way is to predict characters. In this case, we simply convert the sequence of words into its corresponding sequence of characters (using the space '_' as an additional character):\n",
    "\n",
    "`THE CITY OF BOGOTA IN COLOMBIA => ['T','H','E', '_', 'C','I','T','Y','_', 'O', 'F', '_, 'B','O','G','O','T','A','_','I','N','_','C','O','L','O','M','B','I''A']`\n",
    "\n",
    "Key information about using characters as tokens:\n",
    "+ Enough training data for each token, our system would need to predict between 20-30 tokens (depending on the language),\n",
    "+ Out system might generalize to words never seen during training.\n",
    "\n",
    "### **Using words as tokens**\n",
    "Why not predicting full words then? \n",
    "\n",
    "`THE CITY OF BOGOTA IN COLOMBIA => ['THE','CITY','OF','BOGOTA', 'IN', 'COLOMBIA']`\n",
    "\n",
    "Key information about using words as tokens:\n",
    "+ Output sequence is short (only words) and some symbols if defined.\n",
    "+ The system, however, cannot anymore generalize to new words \n",
    "\n",
    "### **Byte Pair Encoding (BPE)**\n",
    "What about something in between? \n",
    "This is what we are trying to do with BPE tokens. BPE is a simple technique inherited from data compression. The basic idea is to allocate tokens for the most frequent sequences of characters. For instance:\n",
    "\n",
    "`THE CITY OF BOGOTA IN COLOMBIA => ['▁TH', 'E', '▁C', 'I', 'TY', '▁OF', '▁BO', 'G', 'O', 'TA', '▁I', 'N', '▁C', 'O', 'L', 'OM', 'B', 'IA']`\n",
    "\n",
    "The [algorithm that finds these tokens](https://en.wikipedia.org/wiki/Byte_pair_encoding) is very simple: we start from the characters and we count how many times two consecutive characters are observed together. We allocate a token for the most frequent pair and we iterate over and over until a specified number of tokens is reached. For more information, you can take a look at [our tutorial on the tokenizers](https://colab.research.google.com/drive/12yE3myHSH-eUxzNM0-FLtEOhzdQoLYWe?usp=sharing).\n",
    "\n",
    "#### *How many BPE tokens should I use?*\n",
    "The number of tokens is one of the hyperparameters of your system.\n",
    "Its optimal value depends on the amount of speech data available. Just to give you an idea, for LibriSpeech (i.e., 1000 hours of sentences in English) a reasonable number of tokens ranges between 1k and 10k.\n",
    "\n",
    "### **Train a Tokenizer**\n",
    "SpeechBrain relies on the popular [SentencePiece](https://github.com/google/sentencepiece) for tokenization. To find the tokens to allocate (given the training transcriptions), run the following code:\n",
    "\n",
    "```\n",
    "cd NLP_Summer_School-2021_Speech_Tutorial/ASR/LibriSpeech/Tokenizer\n",
    "python train.py tokenizer.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Testing the Tokenizer fetched from Speechbrain (HuggingFace hub)**\n",
    "\n",
    "You should be able to fetch the models that you downloaded and then unzipped (they should be in `./pretrained_models/tokenizer.ckpt`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "import speechbrain as sb\n",
    "from speechbrain.pretrained import EncoderDecoderASR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_model = EncoderDecoderASR.from_hparams(source=\"speechbrain/asr-transformer-transformerlm-librispeech\", savedir=\"pretrained_models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tA4HMrnFJ33e",
    "outputId": "4936af97-e07a-4882-b511-bd89b86802b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded as pieces: ['▁THIS', '▁IS', '▁THE', '▁', 'N', 'L', 'P', '▁SUMMER', '▁SCHOOL', ',', '▁THANK', '▁YOU', '▁FOR', '▁ATTEND', 'ING', '▁IT']\n",
      "Encoded as ids: [44, 33, 3, 78, 36, 134, 102, 1321, 761, 0, 868, 24, 25, 1465, 13, 17]\n"
     ]
    }
   ],
   "source": [
    "# Modify the following phrase to check how the Tokenizer works:\n",
    "\n",
    "phrase = 'THIS IS THE NLP SUMMER SCHOOL, THANK YOU FOR ATTENDING IT'\n",
    "print(\"Encoded as pieces: {}\".format(asr_model.tokenizer.encode(phrase, out_type=str)))\n",
    "print(\"Encoded as ids: {}\".format(asr_model.tokenizer.encode_as_ids(phrase)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do you want to know the size of the Tokenizer? \n",
    "print(\"The number of different units in your Tokenizer is: {}\".format(asr_model.tokenizer.vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version 0: ['▁THIS', '▁I', 'S', '▁T', 'HE', '▁', 'N', 'L', 'P', '▁S', 'UM', 'M', 'ER', '▁SCHOOL', ',', '▁TH', 'A', 'N', 'K', '▁', 'Y', 'O', 'U', '▁F', 'OR', '▁AT', 'TE', 'N', 'D', 'IN', 'G', '▁', 'I', 'T']\n",
      "Version 1: ['▁THIS', '▁IS', '▁THE', '▁', 'N', 'L', 'P', '▁SUM', 'M', 'ER', '▁', 'S', 'C', 'H', 'O', 'O', 'L', ',', '▁', 'T', 'HA', 'N', 'K', '▁YOU', '▁FO', 'R', '▁A', 'T', 'TEN', 'D', 'ING', '▁I', 'T']\n",
      "Version 2: ['▁T', 'H', 'I', 'S', '▁I', 'S', '▁T', 'H', 'E', '▁', 'N', 'L', 'P', '▁SUMMER', '▁SC', 'H', 'O', 'O', 'L', ',', '▁THANK', '▁YOU', '▁F', 'O', 'R', '▁ATTEND', 'ING', '▁IT']\n"
     ]
    }
   ],
   "source": [
    "# Nevertheles, there could be several ways how this phrase could be represented:\n",
    "for n in range(3):\n",
    "    print(\"Version {}: {}\".format(n,asr_model.tokenizer.encode(phrase, out_type=str, enable_sampling=True, alpha=0.1, nbest_size=-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is pretty evident that there is a lot of flexibility in how we can represent words (and words sequences) with a BPE-based Tokenizer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PYko19NiKdtK"
   },
   "source": [
    "As mentioned before, we are not training any model in this Tutorial. Nevertheless, we want to share some key insights regarding the training scripts that you might want to know.\n",
    "\n",
    "- Training script: `train.py`,\n",
    "- Hyperparameter file: `tokenizer.yaml`\n",
    "\n",
    "\n",
    "```yaml\n",
    "# ############################################################################\n",
    "# Tokenizer: subword BPE tokenizer with unigram 1K\n",
    "# Training: Mini-LibriSpeech\n",
    "# Authors:  Abdel Heba 2021\n",
    "#           Mirco Ravanelli 2021\n",
    "# ############################################################################\n",
    "\n",
    "\n",
    "# Set up folders for reading from and writing to\n",
    "data_folder: ../data\n",
    "output_folder: ./save\n",
    "\n",
    "# Path where data-specification files are stored\n",
    "train_annotation: ../train.json\n",
    "valid_annotation: ../valid.json\n",
    "test_annotation: ../test.json\n",
    "\n",
    "# Tokenizer parameters\n",
    "token_type: unigram  # [\"unigram\", \"bpe\", \"char\"]\n",
    "token_output: 1000  # index(blank/eos/bos/unk) = 0\n",
    "character_coverage: 1.0\n",
    "annotation_read: words # field to read\n",
    "\n",
    "# Tokenizer object\n",
    "tokenizer: !name:speechbrain.tokenizers.SentencePiece.SentencePiece\n",
    "   model_dir: !ref <output_folder>\n",
    "   vocab_size: !ref <token_output>\n",
    "   annotation_train: !ref <train_annotation>\n",
    "   annotation_read: !ref <annotation_read>\n",
    "   model_type: !ref <token_type> # [\"unigram\", \"bpe\", \"char\"]\n",
    "   character_coverage: !ref <character_coverage>\n",
    "   annotation_list_to_check: [!ref <train_annotation>, !ref <valid_annotation>]\n",
    "   annotation_format: json\n",
    "```\n",
    "\n",
    "The tokenizer is trained on training annotation only. We set here a vocabulary size of 1000. Instead of using the standard BPE algorithm, we use a variation of it based on unigram smoothing. See [sentencepiece](https://github.com/google/sentencepiece) for more info.\n",
    "The tokenizer will be saved in the specified `output_folder`. \n",
    "\n",
    "Let's now take a look into the training script `train.py`:\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Load hyperparameters file with command-line overrides\n",
    "    hparams_file, run_opts, overrides = sb.parse_arguments(sys.argv[1:])\n",
    "    with open(hparams_file) as fin:\n",
    "        hparams = load_hyperpyyaml(fin, overrides)\n",
    "\n",
    "    # Create experiment directory\n",
    "    sb.create_experiment_directory(\n",
    "        experiment_directory=hparams[\"output_folder\"],\n",
    "        hyperparams_to_save=hparams_file,\n",
    "        overrides=overrides,\n",
    "    )\n",
    "\n",
    "    # Data preparation, to be run on only one process.\n",
    "    prepare_mini_librispeech(\n",
    "        data_folder=hparams[\"data_folder\"],\n",
    "        save_json_train=hparams[\"train_annotation\"],\n",
    "        save_json_valid=hparams[\"valid_annotation\"],\n",
    "        save_json_test=hparams[\"test_annotation\"],\n",
    "    )\n",
    "\n",
    "    # Train tokenizer\n",
    "    hparams[\"tokenizer\"]()\n",
    "```\n",
    "\n",
    "Essentially, we prepare the data with the `prepare_mini_librispeech` script and we then run the sentencepiece tokenizer wrapped in \n",
    "`speechbrain.tokenizers.SentencePiece.SentencePiece`.\n",
    "\n",
    "Let's take a look at the files generated by the tokenizer. If you go into the specified output folder (`Tokenizer/save`), you can find two files:\n",
    "+ *1000_unigram.model*\n",
    "+ *1000_unigram.vocab*\n",
    "\n",
    "The first is a binary file containing all the information needed for tokenizing an input text. The second is a text file reporting the list of tokens allocated (with their log probabilities):\n",
    "\n",
    "```\n",
    "▁THE  -3.2458\n",
    "S -3.36618\n",
    "ED  -3.84476\n",
    "▁ -3.91777\n",
    "E -3.92101\n",
    "▁AND  -3.92316\n",
    "▁A  -3.97359\n",
    "▁TO -4.00462\n",
    "▁OF -4.08116\n",
    "....\n",
    "```\n",
    "\n",
    "Let me now show how we can use the learned model to tokenize a text:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ik9hoxBUG03u",
    "outputId": "5440240b-dc87-4715-c363-b9badc970a84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁THE', '▁CITY', '▁OF', '▁MO', 'NT', 'RE', 'AL']\n",
      "[1, 667, 9, 211, 251, 80, 57]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sentencepiece as spm\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"/content/speechbrain/templates/speech_recognition/Tokenizer/save/1000_unigram.model\")\n",
    "\n",
    "# Encode as pieces\n",
    "print(sp.encode_as_pieces('THE CITY OF MONTREAL'))\n",
    "\n",
    "# Encode as ids\n",
    "print(sp.encode_as_ids('THE CITY OF MONTREAL'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oft-7K85LA86"
   },
   "source": [
    "Note that the sentencepiece tokenizers also assign a unique index to each allocated token. These indexes will correspond to the output of our neural networks for language models and ASR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nkYENC7BJ4K9"
   },
   "source": [
    "## **Step 3: Train a Language Model**\n",
    "A Language Model (LM) can be used within a speech recognizer in different ways. In this tutorial, we perform the so-called **shallow fusion** where the language information is used within the beam searcher of the speech recognizer to rescore the partial hypothesis. In practice, for every time step, we rescore the partial hypothesis provided by the speech recognizer with the language scores (that penalize sequences of tokens that are \"unlikely\" to be observed).\n",
    "\n",
    "Some recent studies have shown that a speech recognizer trained on a very large dataset can achieve impressive performance even without a language. However, for medium-scale speech recognition tasks like Librispeech 1000h, the language model still plays a role in improving the final performance.\n",
    "\n",
    "### **Text Corpus**\n",
    "A language model is normally trained on **large text corpora** and it is designed to predict the most probable next token.\n",
    "If you do not have a large text corpus of in-domain data for your application, you might want to skip this part. \n",
    "\n",
    "Another thing to remark is that training a language model on a large text corpus is very **computationally demanding**. You should consider using an available pre-trained model (and maybe fine-tune it). \n",
    "\n",
    "In this tutorial, we train the language model on the training transcriptions of mini-librispeech. This is just to show you how we can train it in a little amount of time. \n",
    "\n",
    "### **Train a LM**\n",
    "\n",
    "We are going to train a simple RNN-based language model that estimates the next tokens given the previous ones.\n",
    "\n",
    "To train it, run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AtMw7x0ybFlI",
    "outputId": "b5a5b366-8e94-4c1d-b989-6c279f5f35b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/d6/a3d2c55b940a7c556e88f5598b401990805fc0f0a28b2fc9870cf0b8c761/datasets-1.6.0-py3-none-any.whl (202kB)\n",
      "\u001b[K     |████████████████████████████████| 204kB 19.5MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyarrow>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.11.1)\n",
      "Requirement already satisfied: huggingface-hub<0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.0.8)\n",
      "Collecting tqdm<4.50.0,>=4.27\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/d5/f220e0c69b2f346b5649b66abebb391df1a00a59997a7ccf823325bd7a3e/tqdm-4.49.0-py2.py3-none-any.whl (69kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 9.2MB/s \n",
      "\u001b[?25hCollecting xxhash\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/4f/0a862cad26aa2ed7a7cd87178cbbfa824fc1383e472d63596a0d018374e7/xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243kB)\n",
      "\u001b[K     |████████████████████████████████| 245kB 54.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (20.9)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
      "Collecting fsspec\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/91/2ef649137816850fa4f4c97c6f2eabb1a79bf0aa2c8ed198e387e373455e/fsspec-2021.4.0-py3-none-any.whl (108kB)\n",
      "\u001b[K     |████████████████████████████████| 112kB 52.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets) (3.10.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
      "Installing collected packages: tqdm, xxhash, fsspec, datasets\n",
      "  Found existing installation: tqdm 4.60.0\n",
      "    Uninstalling tqdm-4.60.0:\n",
      "      Successfully uninstalled tqdm-4.60.0\n",
      "Successfully installed datasets-1.6.0 fsspec-2021.4.0 tqdm-4.49.0 xxhash-2.0.2\n",
      "/content/speechbrain/templates/speech_recognition/LM\n",
      "speechbrain.core - Beginning experiment!\n",
      "speechbrain.core - Experiment folder: results/RNNLM/\n",
      "root - generating datasets...\n",
      "datasets.builder - Using custom data configuration default-5c60079bb1d569f9\n",
      "Downloading and preparing dataset text/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/text/default-5c60079bb1d569f9/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5...\n",
      "Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-5c60079bb1d569f9/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5. Subsequent calls will reuse this data.\n",
      "speechbrain.core - Info: ckpt_interval_minutes arg from hparam file is used\n",
      "speechbrain.core - 2.3M trainable parameters in LM\n",
      "speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n",
      "speechbrain.utils.epoch_loop - Going into epoch 1\n",
      "100% 16/16 [00:00<00:00, 44.50it/s, train_loss=2.23]\n",
      "100% 152/152 [00:00<00:00, 867.02it/s]\n",
      "speechbrain.utils.train_logger - Epoch: 1 - train loss: 2.23 - valid loss: 2.11e-03\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/RNNLM/save/CKPT+2021-04-25+18-35-19+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 2\n",
      "100% 16/16 [00:00<00:00, 98.30it/s, train_loss=0.000365]\n",
      "100% 152/152 [00:00<00:00, 943.61it/s]\n",
      "speechbrain.utils.train_logger - Epoch: 2 - train loss: 3.65e-04 - valid loss: 2.06e-05\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/RNNLM/save/CKPT+2021-04-25+18-35-20+00\n",
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/RNNLM/save/CKPT+2021-04-25+18-35-19+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 3\n",
      "100% 16/16 [00:00<00:00, 97.11it/s, train_loss=1.17e-5]\n",
      "100% 152/152 [00:00<00:00, 925.91it/s]\n",
      "speechbrain.utils.train_logger - Epoch: 3 - train loss: 1.17e-05 - valid loss: 7.27e-06\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/RNNLM/save/CKPT+2021-04-25+18-35-20+01\n",
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/RNNLM/save/CKPT+2021-04-25+18-35-20+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 4\n",
      "100% 16/16 [00:00<00:00, 97.92it/s, train_loss=6.27e-6]\n",
      "100% 152/152 [00:00<00:00, 924.57it/s]\n",
      "speechbrain.utils.train_logger - Epoch: 4 - train loss: 6.27e-06 - valid loss: 5.48e-06\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/RNNLM/save/CKPT+2021-04-25+18-35-20+00\n",
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/RNNLM/save/CKPT+2021-04-25+18-35-20+01\n",
      "speechbrain.utils.epoch_loop - Going into epoch 5\n",
      "100% 16/16 [00:00<00:00, 101.55it/s, train_loss=5.41e-6]\n",
      "100% 152/152 [00:00<00:00, 883.47it/s]\n",
      "speechbrain.utils.train_logger - Epoch: 5 - train loss: 5.41e-06 - valid loss: 5.36e-06\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/RNNLM/save/CKPT+2021-04-25+18-35-21+00\n",
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/RNNLM/save/CKPT+2021-04-25+18-35-20+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 6\n",
      "100% 16/16 [00:00<00:00, 96.14it/s, train_loss=5.36e-6]\n",
      "100% 152/152 [00:00<00:00, 928.57it/s]\n",
      "speechbrain.nnet.schedulers - Changing lr from 0.001 to 0.0008\n",
      "speechbrain.utils.train_logger - Epoch: 6 - train loss: 5.36e-06 - valid loss: 5.36e-06\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/RNNLM/save/CKPT+2021-04-25+18-35-21+01\n",
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/RNNLM/save/CKPT+2021-04-25+18-35-21+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 7\n",
      "100% 16/16 [00:00<00:00, 100.10it/s, train_loss=5.3e-6]\n",
      "100% 152/152 [00:00<00:00, 936.98it/s]\n",
      "speechbrain.utils.train_logger - Epoch: 7 - train loss: 5.30e-06 - valid loss: 5.25e-06\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/RNNLM/save/CKPT+2021-04-25+18-35-21+00\n",
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/RNNLM/save/CKPT+2021-04-25+18-35-21+01\n",
      "speechbrain.utils.epoch_loop - Going into epoch 8\n",
      "100% 16/16 [00:00<00:00, 95.32it/s, train_loss=5.2e-6]\n",
      "100% 152/152 [00:00<00:00, 952.49it/s]\n",
      "speechbrain.utils.train_logger - Epoch: 8 - train loss: 5.20e-06 - valid loss: 5.13e-06\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/RNNLM/save/CKPT+2021-04-25+18-35-22+00\n",
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/RNNLM/save/CKPT+2021-04-25+18-35-21+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 9\n",
      "100% 16/16 [00:00<00:00, 96.86it/s, train_loss=5.13e-6]\n",
      "100% 152/152 [00:00<00:00, 927.75it/s]\n",
      "speechbrain.nnet.schedulers - Changing lr from 0.0008 to 0.00064\n",
      "speechbrain.utils.train_logger - Epoch: 9 - train loss: 5.13e-06 - valid loss: 5.13e-06\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/RNNLM/save/CKPT+2021-04-25+18-35-22+01\n",
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/RNNLM/save/CKPT+2021-04-25+18-35-22+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 10\n",
      "100% 16/16 [00:00<00:00, 98.45it/s, train_loss=5.13e-6]\n",
      "100% 152/152 [00:00<00:00, 940.77it/s]\n",
      "speechbrain.nnet.schedulers - Changing lr from 0.00064 to 0.00051\n",
      "speechbrain.utils.train_logger - Epoch: 10 - train loss: 5.13e-06 - valid loss: 5.13e-06\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/RNNLM/save/CKPT+2021-04-25+18-35-23+00\n",
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/RNNLM/save/CKPT+2021-04-25+18-35-22+01\n",
      "speechbrain.utils.epoch_loop - Going into epoch 11\n",
      "100% 16/16 [00:00<00:00, 94.91it/s, train_loss=5.11e-6]\n",
      "100% 152/152 [00:00<00:00, 912.40it/s]\n",
      "speechbrain.utils.train_logger - Epoch: 11 - train loss: 5.11e-06 - valid loss: 5.01e-06\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/RNNLM/save/CKPT+2021-04-25+18-35-23+01\n",
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/RNNLM/save/CKPT+2021-04-25+18-35-23+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 12\n",
      "100% 16/16 [00:00<00:00, 94.97it/s, train_loss=5.01e-6]\n",
      "100% 152/152 [00:00<00:00, 937.49it/s]\n",
      "speechbrain.nnet.schedulers - Changing lr from 0.00051 to 0.00041\n",
      "speechbrain.utils.train_logger - Epoch: 12 - train loss: 5.01e-06 - valid loss: 5.01e-06\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/RNNLM/save/CKPT+2021-04-25+18-35-23+00\n",
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/RNNLM/save/CKPT+2021-04-25+18-35-23+01\n",
      "speechbrain.utils.epoch_loop - Going into epoch 13\n",
      "100% 16/16 [00:00<00:00, 97.61it/s, train_loss=5.01e-6]\n",
      "100% 152/152 [00:00<00:00, 927.96it/s]\n",
      "speechbrain.nnet.schedulers - Changing lr from 0.00041 to 0.00033\n",
      "speechbrain.utils.train_logger - Epoch: 13 - train loss: 5.01e-06 - valid loss: 5.01e-06\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/RNNLM/save/CKPT+2021-04-25+18-35-24+00\n",
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/RNNLM/save/CKPT+2021-04-25+18-35-23+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 14\n",
      "100% 16/16 [00:00<00:00, 93.65it/s, train_loss=5.01e-6]\n",
      "100% 152/152 [00:00<00:00, 944.65it/s]\n",
      "speechbrain.nnet.schedulers - Changing lr from 0.00033 to 0.00026\n",
      "speechbrain.utils.train_logger - Epoch: 14 - train loss: 5.01e-06 - valid loss: 5.01e-06\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/RNNLM/save/CKPT+2021-04-25+18-35-24+01\n",
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/RNNLM/save/CKPT+2021-04-25+18-35-24+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 15\n",
      "100% 16/16 [00:00<00:00, 96.41it/s, train_loss=5.01e-6]\n",
      "100% 152/152 [00:00<00:00, 936.28it/s]\n",
      "speechbrain.nnet.schedulers - Changing lr from 0.00026 to 0.00021\n",
      "speechbrain.utils.train_logger - Epoch: 15 - train loss: 5.01e-06 - valid loss: 5.01e-06\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/RNNLM/save/CKPT+2021-04-25+18-35-25+00\n",
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/RNNLM/save/CKPT+2021-04-25+18-35-24+01\n",
      "speechbrain.utils.epoch_loop - Going into epoch 16\n",
      "100% 16/16 [00:00<00:00, 97.17it/s, train_loss=5.01e-6]\n",
      "100% 152/152 [00:00<00:00, 926.33it/s]\n",
      "speechbrain.nnet.schedulers - Changing lr from 0.00021 to 0.00017\n",
      "speechbrain.utils.train_logger - Epoch: 16 - train loss: 5.01e-06 - valid loss: 5.01e-06\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/RNNLM/save/CKPT+2021-04-25+18-35-25+01\n",
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/RNNLM/save/CKPT+2021-04-25+18-35-25+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 17\n",
      "100% 16/16 [00:00<00:00, 97.13it/s, train_loss=5.01e-6]\n",
      "100% 152/152 [00:00<00:00, 955.23it/s]\n",
      "speechbrain.nnet.schedulers - Changing lr from 0.00017 to 0.00013\n",
      "speechbrain.utils.train_logger - Epoch: 17 - train loss: 5.01e-06 - valid loss: 5.01e-06\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/RNNLM/save/CKPT+2021-04-25+18-35-26+00\n",
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/RNNLM/save/CKPT+2021-04-25+18-35-25+01\n",
      "speechbrain.utils.epoch_loop - Going into epoch 18\n",
      "100% 16/16 [00:00<00:00, 96.59it/s, train_loss=5.01e-6]\n",
      "100% 152/152 [00:00<00:00, 905.90it/s]\n",
      "speechbrain.nnet.schedulers - Changing lr from 0.00013 to 0.00011\n",
      "speechbrain.utils.train_logger - Epoch: 18 - train loss: 5.01e-06 - valid loss: 5.01e-06\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/RNNLM/save/CKPT+2021-04-25+18-35-26+01\n",
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/RNNLM/save/CKPT+2021-04-25+18-35-26+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 19\n",
      "100% 16/16 [00:00<00:00, 99.25it/s, train_loss=5.01e-6]\n",
      "100% 152/152 [00:00<00:00, 961.88it/s]\n",
      "speechbrain.nnet.schedulers - Changing lr from 0.00011 to 8.6e-05\n",
      "speechbrain.utils.train_logger - Epoch: 19 - train loss: 5.01e-06 - valid loss: 5.01e-06\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/RNNLM/save/CKPT+2021-04-25+18-35-26+00\n",
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/RNNLM/save/CKPT+2021-04-25+18-35-26+01\n",
      "speechbrain.utils.epoch_loop - Going into epoch 20\n",
      "100% 16/16 [00:00<00:00, 99.19it/s, train_loss=5.01e-6]\n",
      "100% 152/152 [00:00<00:00, 930.30it/s]\n",
      "speechbrain.nnet.schedulers - Changing lr from 8.6e-05 to 6.9e-05\n",
      "speechbrain.utils.train_logger - Epoch: 20 - train loss: 5.01e-06 - valid loss: 5.01e-06\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/RNNLM/save/CKPT+2021-04-25+18-35-27+00\n",
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/RNNLM/save/CKPT+2021-04-25+18-35-26+00\n",
      "speechbrain.utils.checkpoints - Loading a checkpoint from results/RNNLM/save/CKPT+2021-04-25+18-35-27+00\n",
      "root - SaveableDataLoader was requested to load a checkpoint, but the data loader has already been iterated. Cannot load checkpoint here. Assuming that the checkpoint was only loaded for e.g. retrieving the best model\n",
      "100% 151/151 [00:00<00:00, 935.60it/s]\n",
      "speechbrain.utils.train_logger - Epoch loaded: 20 - test loss: 5.01e-06\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "%cd /content/speechbrain/templates/speech_recognition/LM\n",
    "!python train.py RNNLM.yaml #--device='cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b3tnXnrWc2My"
   },
   "source": [
    "As you can see from the prints, both the validation and training losses are decreasing. \n",
    "Before diving into the code, let's see which files/folders are generated in the specified `output_folder`:\n",
    "\n",
    "*   `train_log.txt`: contains the statistics (e.g, train_loss, valid_loss) computed at each epoch. \n",
    "*   `log.txt`: is a more detailed logger containing the timestamps for each basic operation.\n",
    "*  `env.log`: shows all the dependencies used with their corresponding version (useful for replicability).\n",
    "\n",
    "*  `train.py`, `hyperparams.yaml`:  are a copy of the experiment file along with the corresponding hyperparameters (for replicability).\n",
    "\n",
    "* `save`:  is the place where we store the learned model.\n",
    "\n",
    "In the `save` folder, you find subfolders containing the checkpoints saved during training (in the format `CKPT+data+time`). Typically, you find here two checkpoints: the best (i.e, the oldest one) and the latest (i.e, the most recent one). If you find only a single checkpoint it means that the last epoch is also the best.\n",
    "\n",
    "Inside each checkpoint, you can find all the information needed to resume training (e.g, models, optimizers, schedulers, epoch counter, etc.). The parameters of the RNNLM model are reported in `model.ckpt` file. This is just a binary format readable with `torch.load`.\n",
    "\n",
    "\n",
    "As usual, we have a `train.py` and a hyperparameter file called `RNNLM.yaml`. \n",
    "\n",
    "### **Hyperparameters**\n",
    "[You can take a look into the full RNNLM.yaml file here](https://github.com/speechbrain/speechbrain/blob/develop/templates/speech_recognition/LM/RNNLM.yaml).\n",
    "\n",
    "In the first part, we specify some basic settings, such as the seed, the path of the output folders and the training logger:\n",
    "\n",
    "```yaml\n",
    "seed: 2602\n",
    "__set_seed: !apply:torch.manual_seed [!ref <seed>]\n",
    "output_folder: !ref results/RNNLM/\n",
    "save_folder: !ref <output_folder>/save\n",
    "train_log: !ref <output_folder>/train_log.txt\n",
    "```\n",
    "\n",
    "We then specify the path of the text corpora used for training, validation, and test:\n",
    "\n",
    "```yaml\n",
    "lm_train_data: data/train.txt\n",
    "lm_valid_data: data/valid.txt\n",
    "lm_test_data: data/test.txt\n",
    "```\n",
    "\n",
    "Different from all the other recipes, the LM one directly reads big corpora in raw text format (without the need for the JSON/CSV files). This is done with the [HuggingFace dataset](https://huggingface.co/), that turned out to be very efficient and easy to use.\n",
    "\n",
    "Next, we set up the train_logger and we specify which tokenizer we use to transform the input words into a sequence of tokens. In this case, we have to use the tokenizer trained at the previous step:\n",
    "\n",
    "```yaml\n",
    "# The train logger writes training statistics to a file, as well as stdout.\n",
    "train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n",
    "    save_file: !ref <train_log>\n",
    "\n",
    "# Tokenizer model (you must use the same tokenizer for LM and ASR training)\n",
    "tokenizer_file: ../Tokenizer/save/1000_unigram.model\n",
    "```\n",
    "\n",
    "\n",
    "We can now specify some training hyperparameters such as the number of epochs, the batch size, and the learning rate. We also define the most important architectural hyperparameters (e.g, number of layers, number of neurons per layer, output dimensionality).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```yaml\n",
    "# Training parameters\n",
    "number_of_epochs: 20\n",
    "batch_size: 80\n",
    "lr: 0.001\n",
    "accu_steps: 1 # Gradient accumulation to simulate large batch training\n",
    "ckpt_interval_minutes: 15 # save checkpoint every N min\n",
    "\n",
    "# Dataloader options\n",
    "train_dataloader_opts:\n",
    "    batch_size: !ref <batch_size>\n",
    "    shuffle: True\n",
    "\n",
    "valid_dataloader_opts:\n",
    "    batch_size: 1\n",
    "\n",
    "test_dataloader_opts:\n",
    "    batch_size: 1\n",
    "\n",
    "# Model parameters\n",
    "emb_dim: 256 # dimension of the embeddings\n",
    "rnn_size: 512 # dimension of hidden layers\n",
    "layers: 2 # number of hidden layers\n",
    "\n",
    "# Outputs\n",
    "output_neurons: 1000 # index(eos/bos) = 0\n",
    "```\n",
    "\n",
    "Next, we define the objects that we will use to train our language model. We thus declare objects for the RNN model, the cost function, the optimizer, and the learning rate scheduler:\n",
    "\n",
    "\n",
    "```yaml\n",
    "model: !new:templates.speech_recognition.LM.custom_model.CustomModel\n",
    "    embedding_dim: !ref <emb_dim>\n",
    "    rnn_size: !ref <rnn_size>\n",
    "    layers: !ref <layers>\n",
    "\n",
    "\n",
    "# Cost function used for training the model\n",
    "compute_cost: !name:speechbrain.nnet.losses.nll_loss\n",
    "\n",
    "# This optimizer will be constructed by the Brain class after all parameters\n",
    "# are moved to the correct device. Then it will be added to the checkpointer.\n",
    "optimizer: !name:torch.optim.Adam\n",
    "    lr: !ref <lr>\n",
    "    betas: (0.9, 0.98)\n",
    "    eps: 0.000000001\n",
    "\n",
    "# This function manages learning rate annealing over the epochs.\n",
    "# We here use the NewBoB algorithm, that anneals the learning rate if\n",
    "# the improvements over two consecutive epochs is less than the defined\n",
    "# threshold.\n",
    "lr_annealing: !new:speechbrain.nnet.schedulers.NewBobScheduler\n",
    "    initial_value: !ref <lr>\n",
    "    improvement_threshold: 0.0025\n",
    "    annealing_factor: 0.8\n",
    "    patient: 0\n",
    "```\n",
    "The model that we used in this example is defined in the `custom_model.py` file. As mentioned, this is just a simple RNN but users can easily plug here their custom models (e.g .convolutional models or Transformers). \n",
    "\n",
    "We conclude the hyperparameter specification with the declaration of the epoch counter, tokenizer, and checkpointer:\n",
    "\n",
    "\n",
    "```yaml\n",
    "# The first object passed to the Brain class is this \"Epoch Counter\"\n",
    "# which is saved by the Checkpointer so that training can be resumed\n",
    "# if it gets interrupted at any point.\n",
    "epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n",
    "    limit: !ref <number_of_epochs>\n",
    "\n",
    "# Objects in \"modules\" dict will have their parameters moved to the correct\n",
    "# device, as well as having train()/eval() called on them by the Brain class.\n",
    "modules:\n",
    "    model: !ref <model>\n",
    "\n",
    "# Tokenier initialization\n",
    "tokenizer: !new:sentencepiece.SentencePieceProcessor\n",
    "\n",
    "# This object is used for saving the state of training both so that it\n",
    "# can be resumed if it gets interrupted, and also so that the best checkpoint\n",
    "# can be later loaded for evaluation or inference.\n",
    "checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n",
    "    checkpoints_dir: !ref <save_folder>\n",
    "    recoverables:\n",
    "        model: !ref <model>\n",
    "        scheduler: !ref <lr_annealing>\n",
    "        counter: !ref <epoch_counter>\n",
    "\n",
    "# Pretrain the tokenizer\n",
    "pretrainer: !new:speechbrain.utils.parameter_transfer.Pretrainer\n",
    "    loadables:\n",
    "        tokenizer: !ref <tokenizer>\n",
    "    paths:\n",
    "        tokenizer: !ref <tokenizer_file>\n",
    "```\n",
    "\n",
    "The last class is the pre-trainer, which connects the tokenizer object with the specified pre-trained tokenizer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mnCM5xuy85P4"
   },
   "source": [
    "### **Experiment file**\n",
    "Let's now take a look into how the objects, functions, and hyperparameters declared in the yaml file are used in `train.py` to implement the language model.\n",
    "\n",
    "Let's start from the main of the `train.py`:\n",
    "\n",
    "\n",
    "```python\n",
    "# Recipe begins!\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Reading command line arguments\n",
    "    hparams_file, run_opts, overrides = sb.parse_arguments(sys.argv[1:])\n",
    "\n",
    "    # Initialize ddp (useful only for multi-GPU DDP training)\n",
    "    sb.utils.distributed.ddp_init_group(run_opts)\n",
    "\n",
    "    # Load hyperparameters file with command-line overrides\n",
    "    with open(hparams_file) as fin:\n",
    "        hparams = load_hyperpyyaml(fin, overrides)\n",
    "\n",
    "    # Create experiment directory\n",
    "    sb.create_experiment_directory(\n",
    "        experiment_directory=hparams[\"output_folder\"],\n",
    "        hyperparams_to_save=hparams_file,\n",
    "        overrides=overrides,\n",
    "    )\n",
    "```\n",
    "\n",
    "We here do some preliminary operations such as parsing the command line, initializing the distributed data-parallel (needed if multiple GPUs are used), creating the output folder, and reading the yaml file.\n",
    "\n",
    "After reading the yaml file with `load_hyperpyyaml`, all the objects declared in the hyperparameter files are initialized and available in a dictionary form (along with the other functions and parameters reported in the yaml file).\n",
    "For instance,  we will have `hparams['model']`, `hparams['optimizer']`, `hparams['batch_size']`, etc.\n",
    "\n",
    "\n",
    "#### **Data-IO Pipeline**\n",
    "We then call a special function that creates the dataset objects for training, validation, and test.\n",
    "\n",
    "```python\n",
    "    # Create dataset objects \"train\", \"valid\", and \"test\"\n",
    "    train_data, valid_data, test_data = dataio_prepare(hparams)\n",
    "```\n",
    "\n",
    "Let's take a closer look into that.\n",
    "\n",
    "\n",
    "```python\n",
    "def dataio_prepare(hparams):\n",
    "    \"\"\"This function prepares the datasets to be used in the brain class.\n",
    "    It also defines the data processing pipeline through user-defined functions.\n",
    "\n",
    "    The language model is trained with the text files specified by the user in\n",
    "    the hyperparameter file.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    hparams : dict\n",
    "        This dictionary is loaded from the `train.yaml` file, and it includes\n",
    "        all the hyperparameters needed for dataset construction and loading.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    datasets : list\n",
    "        List containing \"train\", \"valid\", and \"test\" sets that correspond\n",
    "        to the appropriate DynamicItemDataset object.\n",
    "    \"\"\"\n",
    "\n",
    "    logging.info(\"generating datasets...\")\n",
    "\n",
    "    # Prepare datasets\n",
    "    datasets = load_dataset(\n",
    "        \"text\",\n",
    "        data_files={\n",
    "            \"train\": hparams[\"lm_train_data\"],\n",
    "            \"valid\": hparams[\"lm_valid_data\"],\n",
    "            \"test\": hparams[\"lm_test_data\"],\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Convert huggingface's dataset to DynamicItemDataset via a magical function\n",
    "    train_data = sb.dataio.dataset.DynamicItemDataset.from_arrow_dataset(\n",
    "        datasets[\"train\"]\n",
    "    )\n",
    "    valid_data = sb.dataio.dataset.DynamicItemDataset.from_arrow_dataset(\n",
    "        datasets[\"valid\"]\n",
    "    )\n",
    "    test_data = sb.dataio.dataset.DynamicItemDataset.from_arrow_dataset(\n",
    "        datasets[\"test\"]\n",
    "    )\n",
    "\n",
    "    datasets = [train_data, valid_data, test_data]\n",
    "    tokenizer = hparams[\"tokenizer\"]\n",
    "\n",
    "    # Define text processing pipeline. We start from the raw text and then\n",
    "    # encode it using the tokenizer. The tokens with bos are used for feeding\n",
    "    # the neural network, the tokens with eos for computing the cost function.\n",
    "    @sb.utils.data_pipeline.takes(\"text\")\n",
    "    @sb.utils.data_pipeline.provides(\"text\", \"tokens_bos\", \"tokens_eos\")\n",
    "    def text_pipeline(text):\n",
    "        yield text\n",
    "        tokens_list = tokenizer.encode_as_ids(text)\n",
    "        tokens_bos = torch.LongTensor([hparams[\"bos_index\"]] + (tokens_list))\n",
    "        yield tokens_bos\n",
    "        tokens_eos = torch.LongTensor(tokens_list + [hparams[\"eos_index\"]])\n",
    "        yield tokens_eos\n",
    "\n",
    "    sb.dataio.dataset.add_dynamic_item(datasets, text_pipeline)\n",
    "\n",
    "    # 4. Set outputs to add into the batch. The batch variable will contain\n",
    "    # all these fields (e.g, batch.id, batch.text, batch.tokens.bos,..)\n",
    "    sb.dataio.dataset.set_output_keys(\n",
    "        datasets, [\"id\", \"text\", \"tokens_bos\", \"tokens_eos\"],\n",
    "    )\n",
    "    return train_data, valid_data, test_data\n",
    "```\n",
    "\n",
    "The first part is just a conversion from the HuggingFace dataset to the DynamicItemDataset used in SpeechBrain. \n",
    "\n",
    "You can notice that we expose the text processing function `text_pipeline`, which takes in input the text of one sentence and processes it in different ways. \n",
    "\n",
    "The text processing function converts the raw text into the corresponding tokens (in index form). We also create other variables such as the version of the sequence with the beginning of the sentence `<bos>`  token in front and the one with the end of sentence `<eos>` as the last element. Their usefulness will be clear later.\n",
    "\n",
    "Before returning the dataset objects, the `dataio_prepare` specifies which keys we would like to output. As we will see later, these keys will be available in the brain class as `batch.id`, `batch.text`, `batch.tokens_bos`, etc.\n",
    "[For more information on the data loader, please take a look into this tutorial](https://colab.research.google.com/drive/1AiVJZhZKwEI4nFGANKXEe-ffZFfvXKwH?usp=sharing)\n",
    "\n",
    "\n",
    "After the definition of the datasets, the main function can go ahead with the  initialization of the brain class:\n",
    "\n",
    "```python\n",
    "    # Initialize the Brain object to prepare for LM training.\n",
    "    lm_brain = LM(\n",
    "        modules=hparams[\"modules\"],\n",
    "        opt_class=hparams[\"optimizer\"],\n",
    "        hparams=hparams,\n",
    "        run_opts=run_opts,\n",
    "        checkpointer=hparams[\"checkpointer\"],\n",
    "    )\n",
    "```\n",
    "The brain class implements all the functionalities needed for supporting the training and validation loops.  Its `fit` and `evaluate` methods perform training and test, respectively:\n",
    "\n",
    "```python\n",
    "    lm_brain.fit(\n",
    "        lm_brain.hparams.epoch_counter,\n",
    "        train_data,\n",
    "        valid_data,\n",
    "        train_loader_kwargs=hparams[\"train_dataloader_opts\"],\n",
    "        valid_loader_kwargs=hparams[\"valid_dataloader_opts\"],\n",
    "    )\n",
    "\n",
    "    # Load best checkpoint for evaluation\n",
    "    test_stats = lm_brain.evaluate(\n",
    "        test_data,\n",
    "        min_key=\"loss\",\n",
    "        test_loader_kwargs=hparams[\"test_dataloader_opts\"],\n",
    "    )\n",
    "```\n",
    "The training and validation data loaders are given in input to the fit method, while the test dataset is fed into the evaluate method.\n",
    "\n",
    "Let's now take a look into the most important methods defined in the brain class.\n",
    "\n",
    "#### **Forward Computations**\n",
    "\n",
    "Let's start with the `forward` function, which defines all the computations needed to transform the input text into the output predictions.\n",
    "\n",
    "\n",
    "```python\n",
    "    def compute_forward(self, batch, stage):\n",
    "        \"\"\"Predicts the next word given the previous ones.\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        batch : PaddedBatch\n",
    "            This batch object contains all the relevant tensors for computation.\n",
    "        stage : sb.Stage\n",
    "            One of sb.Stage.TRAIN, sb.Stage.VALID, or sb.Stage.TEST.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        predictions : torch.Tensor\n",
    "            A tensor containing the posterior probabilities (predictions).\n",
    "        \"\"\"\n",
    "        batch = batch.to(self.device)\n",
    "        tokens_bos, _ = batch.tokens_bos\n",
    "        pred = self.hparams.model(tokens_bos)\n",
    "        return pred\n",
    "```\n",
    "\n",
    "In this case, the chain of computation is very simple. We just put the batch on the right device and feed the encoded tokens into the model. We feed the tokens with `<bos>` into the model.\n",
    "When adding the `<bos>` token, in fact, we shift all the tokens by one element. This way, our input corresponds to the previous token while our model tries to predict the current one.\n",
    "\n",
    "#### **Compute Objectives**\n",
    "\n",
    "Let's take a look now into the `compute_objectives` method that takes in input the targets, the predictions, and estimates a loss function:\n",
    "\n",
    "```python\n",
    "    def compute_objectives(self, predictions, batch, stage):\n",
    "        \"\"\"Computes the loss given the predicted and targeted outputs.\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        predictions : torch.Tensor\n",
    "            The posterior probabilities from `compute_forward`.\n",
    "        batch : PaddedBatch\n",
    "            This batch object contains all the relevant tensors for computation.\n",
    "        stage : sb.Stage\n",
    "            One of sb.Stage.TRAIN, sb.Stage.VALID, or sb.Stage.TEST.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss : torch.Tensor\n",
    "            A one-element tensor used for backpropagating the gradient.\n",
    "        \"\"\"\n",
    "        batch = batch.to(self.device)\n",
    "        tokens_eos, tokens_len = batch.tokens_eos\n",
    "        loss = self.hparams.compute_cost(\n",
    "            predictions, tokens_eos, length=tokens_len\n",
    "        )\n",
    "        return loss\n",
    "```\n",
    "The predictions are those computed in the forward method. The cost function is evaluated by comparing these predictions with the target tokens. We here use the tokens with the special `<eos>` token at the end because we want to predict when the sentence ends as well.\n",
    "\n",
    "####**Other methods**\n",
    "Beyond these two important functions, we have some other methods that are used by the brain class. In particular, the `fit_batch` trains each batch of data (by computing the gradient with the backward method and the updates with step one). The `on_stage_end`, is called at the end of each stage (e.g, at the end of each training epoch) and mainly takes care of statistic management, learning rate annealing, and checkpointing. [For a more detailed description of the brain class, please take a look into this tutorial](https://colab.research.google.com/drive/12bg3aUdr9mTfOGqcB5pSMABoIKPgiwcM?usp=sharing). For more information on checkpointing, [take a look here](https://colab.research.google.com/drive/1VH7U0oP3CZsUNtChJT2ewbV_q1QX8xre?usp=sharing)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tFJ34alleSBH"
   },
   "source": [
    "## **Step 4: Speech Recognizer**\n",
    "At this point, we can train our speech recognizer. In this tutorial, we are\n",
    "going to train an **attention-based end-to-end speech recognizer** (offline).\n",
    "The encoder relies on a combination of convolutional, recurrent, and fully connected models. The decoder is an autoregressive GRU decoder. An attention mechanism is employed between encoding and decoder. The final sequence of words is retrieved with beamsearch coupled with the RNNLM trained in the previous step. \n",
    "The attention-based system is jointly trained with CTC (applied on the top of the encoder).\n",
    "The system uses data augmentation techniques to improve its performance.\n",
    "\n",
    "### **Train the speech recognizer**\n",
    "To train the speech recognizer, run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "55c4jnVCeoGa",
    "outputId": "c9693bb5-e047-4d03-83af-a530fafac6c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/speechbrain/templates/speech_recognition/ASR\n",
      "Downloading http://www.openslr.org/resources/28/rirs_noises.zip to ../data/rirs_noises.zip\n",
      "rirs_noises.zip: 1.31GB [00:42, 30.8MB/s]                \n",
      "Extracting ../data/rirs_noises.zip to ../data\n",
      "speechbrain.core - Beginning experiment!\n",
      "speechbrain.core - Experiment folder: results/CRDNN_BPE_960h_LM/2602\n",
      "mini_librispeech_prepare - Preparation completed in previous run, skipping.\n",
      "speechbrain.pretrained.fetching - Fetch lm.ckpt: Delegating to Huggingface hub, source speechbrain/asr-crdnn-rnnlm-librispeech.\n",
      "filelock - Lock 140130605497168 acquired on /root/.cache/huggingface/hub/651df066b5d0b2efef7208f51df93d3a0a65bedc3a3a2500cd7b8faf064e631e.b438b9af3f549a23c4458bb066c11cd51dc1cfe9bfef30d3eb66b472e93b1e8c.lock\n",
      "huggingface_hub.file_download - downloading https://huggingface.co/speechbrain/asr-crdnn-rnnlm-librispeech/resolve/main/lm.ckpt to /root/.cache/huggingface/hub/tmpps__p7vx\n",
      "Downloading: 100% 212M/212M [00:04<00:00, 50.2MB/s]\n",
      "huggingface_hub.file_download - storing https://huggingface.co/speechbrain/asr-crdnn-rnnlm-librispeech/resolve/main/lm.ckpt in cache at /root/.cache/huggingface/hub/651df066b5d0b2efef7208f51df93d3a0a65bedc3a3a2500cd7b8faf064e631e.b438b9af3f549a23c4458bb066c11cd51dc1cfe9bfef30d3eb66b472e93b1e8c\n",
      "huggingface_hub.file_download - creating metadata file for /root/.cache/huggingface/hub/651df066b5d0b2efef7208f51df93d3a0a65bedc3a3a2500cd7b8faf064e631e.b438b9af3f549a23c4458bb066c11cd51dc1cfe9bfef30d3eb66b472e93b1e8c\n",
      "filelock - Lock 140130605497168 released on /root/.cache/huggingface/hub/651df066b5d0b2efef7208f51df93d3a0a65bedc3a3a2500cd7b8faf064e631e.b438b9af3f549a23c4458bb066c11cd51dc1cfe9bfef30d3eb66b472e93b1e8c.lock\n",
      "speechbrain.pretrained.fetching - Fetch tokenizer.ckpt: Delegating to Huggingface hub, source speechbrain/asr-crdnn-rnnlm-librispeech.\n",
      "filelock - Lock 140130625758928 acquired on /root/.cache/huggingface/hub/f39208eba495042a59a8404b5703ca08a39a85e4d2bf707e197b90a3323f92ab.cd7af7ea8cfcfbf0f6dd61514c361972eb82b3b76f12b0e9ee0b371f36fdc078.lock\n",
      "huggingface_hub.file_download - downloading https://huggingface.co/speechbrain/asr-crdnn-rnnlm-librispeech/resolve/main/tokenizer.ckpt to /root/.cache/huggingface/hub/tmpp7u_v4eo\n",
      "Downloading: 100% 253k/253k [00:00<00:00, 38.5MB/s]\n",
      "huggingface_hub.file_download - storing https://huggingface.co/speechbrain/asr-crdnn-rnnlm-librispeech/resolve/main/tokenizer.ckpt in cache at /root/.cache/huggingface/hub/f39208eba495042a59a8404b5703ca08a39a85e4d2bf707e197b90a3323f92ab.cd7af7ea8cfcfbf0f6dd61514c361972eb82b3b76f12b0e9ee0b371f36fdc078\n",
      "huggingface_hub.file_download - creating metadata file for /root/.cache/huggingface/hub/f39208eba495042a59a8404b5703ca08a39a85e4d2bf707e197b90a3323f92ab.cd7af7ea8cfcfbf0f6dd61514c361972eb82b3b76f12b0e9ee0b371f36fdc078\n",
      "filelock - Lock 140130625758928 released on /root/.cache/huggingface/hub/f39208eba495042a59a8404b5703ca08a39a85e4d2bf707e197b90a3323f92ab.cd7af7ea8cfcfbf0f6dd61514c361972eb82b3b76f12b0e9ee0b371f36fdc078.lock\n",
      "speechbrain.pretrained.fetching - Fetch asr.ckpt: Delegating to Huggingface hub, source speechbrain/asr-crdnn-rnnlm-librispeech.\n",
      "filelock - Lock 140130613059088 acquired on /root/.cache/huggingface/hub/83e944252a91fe1d0883daa1e87077df4d64c35fffb45e22fff924faace4a59c.7fdf4aabd8400c69a6228ccc17c83b7a8ebf34c5d76f23497b7cf0d7a1baaea3.lock\n",
      "huggingface_hub.file_download - downloading https://huggingface.co/speechbrain/asr-crdnn-rnnlm-librispeech/resolve/main/asr.ckpt to /root/.cache/huggingface/hub/tmpvvyufycq\n",
      "Downloading: 100% 480M/480M [00:07<00:00, 64.0MB/s]\n",
      "huggingface_hub.file_download - storing https://huggingface.co/speechbrain/asr-crdnn-rnnlm-librispeech/resolve/main/asr.ckpt in cache at /root/.cache/huggingface/hub/83e944252a91fe1d0883daa1e87077df4d64c35fffb45e22fff924faace4a59c.7fdf4aabd8400c69a6228ccc17c83b7a8ebf34c5d76f23497b7cf0d7a1baaea3\n",
      "huggingface_hub.file_download - creating metadata file for /root/.cache/huggingface/hub/83e944252a91fe1d0883daa1e87077df4d64c35fffb45e22fff924faace4a59c.7fdf4aabd8400c69a6228ccc17c83b7a8ebf34c5d76f23497b7cf0d7a1baaea3\n",
      "filelock - Lock 140130613059088 released on /root/.cache/huggingface/hub/83e944252a91fe1d0883daa1e87077df4d64c35fffb45e22fff924faace4a59c.7fdf4aabd8400c69a6228ccc17c83b7a8ebf34c5d76f23497b7cf0d7a1baaea3.lock\n",
      "speechbrain.utils.parameter_transfer - Loading pretrained files for: lm, tokenizer, model\n",
      "speechbrain.core - Info: ckpt_interval_minutes arg from hparam file is used\n",
      "speechbrain.core - 173.0M trainable parameters in ASR\n",
      "speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n",
      "speechbrain.utils.epoch_loop - Going into epoch 1\n",
      " 89% 677/760 [12:35<02:01,  1.47s/it, train_loss=0.953]"
     ]
    }
   ],
   "source": [
    "%cd /content/speechbrain/templates/speech_recognition/ASR\n",
    "!python train.py train.yaml --batch_size=2 #--device='cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-JCc1qyXpFI"
   },
   "source": [
    "Running this code might take quite a bit on google Colab. As you can see from the log,  the loss is progressively improving after each epoch. \n",
    "The specified  `output_folder` will contain the same files and folders already discussed in the RNNLM part. In addition to that, we save a file called `wer.txt` that reports the word-error-rate achieved for every test sentence (along with the corresponding alignment with the true transcription):\n",
    "\n",
    "\n",
    "```\n",
    "%WER 3.09 [ 1622 / 52576, 167 ins, 171 del, 1284 sub ]\n",
    "%SER 33.66 [ 882 / 2620 ]\n",
    "Scored 2620 sentences, 0 not present in hyp.\n",
    "================================================================================\n",
    "ALIGNMENTS\n",
    "\n",
    "Format:\n",
    "<utterance-id>, WER DETAILS\n",
    "<eps> ; reference  ; on ; the ; first ;  line\n",
    "  I   ;     S      ; =  ;  =  ;   S   ;   D  \n",
    " and  ; hypothesis ; on ; the ; third ; <eps>\n",
    "================================================================================\n",
    "672-122797-0033, %WER 0.00 [ 0 / 2, 0 ins, 0 del, 0 sub ]\n",
    "A ; STORY\n",
    "= ;   =  \n",
    "A ; STORY\n",
    "================================================================================\n",
    "2094-142345-0041, %WER 0.00 [ 0 / 1, 0 ins, 0 del, 0 sub ]\n",
    "DIRECTION\n",
    "    =    \n",
    "DIRECTION\n",
    "================================================================================\n",
    "2830-3980-0026, %WER 50.00 [ 1 / 2, 0 ins, 0 del, 1 sub ]\n",
    "VERSE ; TWO\n",
    "  S   ;  = \n",
    "FIRST ; TWO\n",
    "================================================================================\n",
    "237-134500-0025, %WER 50.00 [ 1 / 2, 0 ins, 0 del, 1 sub ]\n",
    "OH ;  EMIL\n",
    "=  ;   S  \n",
    "OH ; AMIEL\n",
    "================================================================================\n",
    "7127-75947-0012, %WER 0.00 [ 0 / 2, 0 ins, 0 del, 0 sub ]\n",
    "INDEED ; AH\n",
    "  =    ; = \n",
    "INDEED ; AH\n",
    "================================================================================\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "Let's now take a closer look into the hyperparameter (`train.yaml`)  and experiment script (`train.py`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfHa8TQMYUle"
   },
   "source": [
    "### **Hyperparameters**\n",
    "\n",
    "The hyperparameter file starts with the definition of basic things, such as seed and path settings:\n",
    "\n",
    "```yaml\n",
    "# Seed needs to be set at top of yaml, before objects with parameters are instantiated\n",
    "seed: 2602\n",
    "__set_seed: !apply:torch.manual_seed [!ref <seed>]\n",
    "\n",
    "data_folder: ../data # In this case, data will be automatically downloaded here.\n",
    "data_folder_rirs: !ref <data_folder> # noise/ris dataset will automatically be downloaded here\n",
    "output_folder: !ref results/CRDNN_BPE_960h_LM/<seed>\n",
    "wer_file: !ref <output_folder>/wer.txt\n",
    "save_folder: !ref <output_folder>/save\n",
    "train_log: !ref <output_folder>/train_log.txt\n",
    "\n",
    "pretrained_path: speechbrain/asr-crdnn-rnnlm-librispeech\n",
    "\n",
    "# Path where data manifest files will be stored. The data manifest files are created by the\n",
    "# data preparation script\n",
    "train_annotation: ../train.json\n",
    "valid_annotation: ../valid.json\n",
    "test_annotation: ../test.json\n",
    "\n",
    "# The train logger writes training statistics to a file, as well as stdout.\n",
    "train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n",
    "    save_file: !ref <train_log>\n",
    "```\n",
    "\n",
    "The `data_folder` corresponds to the path where the mini-librispeech is stored. If not available, the mini-librispeech dataset will be downloaded here. As mentioned, the script also supports data augmentation. To do it, we use the impulse responses and noise sequences of the open rir dataset (again, if not available it will be downloaded here).\n",
    "\n",
    "We also specify the folder where the language model is saved. In this case, we use the official pre-trained language model available on HuggingFace, but you can change it and use the one trained at the previous step (you should point to the checkpoint in the folder where the best `model.cpkt` is stored).\n",
    "What is important is that the set of tokens used for the LM and the one used for training the speech recognizer match exactly. \n",
    "\n",
    "We also have to specify the data manifest files for training, validation, and test. If not available, these files will be created by the data preparation script called in `train.py`.\n",
    "\n",
    "After that, we define a bunch of parameters for training, feature extraction, model definition, and decoding:\n",
    "\n",
    "```yaml\n",
    "# Training parameters\n",
    "number_of_epochs: 15\n",
    "number_of_ctc_epochs: 5\n",
    "batch_size: 8\n",
    "lr: 1.0\n",
    "ctc_weight: 0.5\n",
    "sorting: ascending\n",
    "ckpt_interval_minutes: 15 # save checkpoint every N min\n",
    "label_smoothing: 0.1\n",
    "\n",
    "# Dataloader options\n",
    "train_dataloader_opts:\n",
    "    batch_size: !ref <batch_size>\n",
    "\n",
    "valid_dataloader_opts:\n",
    "    batch_size: !ref <batch_size>\n",
    "\n",
    "test_dataloader_opts:\n",
    "    batch_size: !ref <batch_size>\n",
    "\n",
    "\n",
    "# Feature parameters\n",
    "sample_rate: 16000\n",
    "n_fft: 400\n",
    "n_mels: 40\n",
    "\n",
    "# Model parameters\n",
    "activation: !name:torch.nn.LeakyReLU\n",
    "dropout: 0.15\n",
    "cnn_blocks: 2\n",
    "cnn_channels: (128, 256)\n",
    "inter_layer_pooling_size: (2, 2)\n",
    "cnn_kernelsize: (3, 3)\n",
    "time_pooling_size: 4\n",
    "rnn_class: !name:speechbrain.nnet.RNN.LSTM\n",
    "rnn_layers: 4\n",
    "rnn_neurons: 1024\n",
    "rnn_bidirectional: True\n",
    "dnn_blocks: 2\n",
    "dnn_neurons: 512\n",
    "emb_size: 128\n",
    "dec_neurons: 1024\n",
    "output_neurons: 1000  # Number of tokens (same as LM)\n",
    "blank_index: 0\n",
    "bos_index: 0\n",
    "eos_index: 0\n",
    "unk_index: 0\n",
    "\n",
    "# Decoding parameters\n",
    "min_decode_ratio: 0.0\n",
    "max_decode_ratio: 1.0\n",
    "valid_beam_size: 8\n",
    "test_beam_size: 80\n",
    "eos_threshold: 1.5\n",
    "using_max_attn_shift: True\n",
    "max_attn_shift: 240\n",
    "lm_weight: 0.50\n",
    "ctc_weight_decode: 0.0\n",
    "coverage_penalty: 1.5\n",
    "temperature: 1.25\n",
    "temperature_lm: 1.25\n",
    "```\n",
    "\n",
    "For instance, we define the number of epochs, the initial learning rate, the batch size, the weight of the CTC loss, and many others. \n",
    "\n",
    "By setting sorting to `ascending`, we sort all the sentences in ascending order before creating the batches. This minimizes the need for zero paddings and thus makes training faster without losing performance (at least in this task with this model). \n",
    "\n",
    "Many other parameters are defined. For the exact meaning of all of them, you can refer to the docstring of the function/class using this hyperparameter.\n",
    "\n",
    "In the next block, we define the most important classes that are needed to implement the speech recognizer:\n",
    "\n",
    "\n",
    "```yaml\n",
    "# The first object passed to the Brain class is this \"Epoch Counter\"\n",
    "# which is saved by the Checkpointer so that training can be resumed\n",
    "# if it gets interrupted at any point.\n",
    "epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n",
    "    limit: !ref <number_of_epochs>\n",
    "\n",
    "# Feature extraction\n",
    "compute_features: !new:speechbrain.lobes.features.Fbank\n",
    "    sample_rate: !ref <sample_rate>\n",
    "    n_fft: !ref <n_fft>\n",
    "    n_mels: !ref <n_mels>\n",
    "\n",
    "# Feature normalization (mean and std)\n",
    "normalize: !new:speechbrain.processing.features.InputNormalization\n",
    "    norm_type: global\n",
    "\n",
    "# Added noise and reverb come from OpenRIR dataset, automatically\n",
    "# downloaded and prepared with this Environmental Corruption class.\n",
    "env_corrupt: !new:speechbrain.lobes.augment.EnvCorrupt\n",
    "    openrir_folder: !ref <data_folder_rirs>\n",
    "    babble_prob: 0.0\n",
    "    reverb_prob: 0.0\n",
    "    noise_prob: 1.0\n",
    "    noise_snr_low: 0\n",
    "    noise_snr_high: 15\n",
    "\n",
    "# Adds speech change + time and frequnecy dropouts (time-domain implementation).\n",
    "augmentation: !new:speechbrain.lobes.augment.TimeDomainSpecAugment\n",
    "    sample_rate: !ref <sample_rate>\n",
    "    speeds: [95, 100, 105]\n",
    "\n",
    "# The CRDNN model is an encoder that combines CNNs, RNNs, and DNNs.\n",
    "encoder: !new:speechbrain.lobes.models.CRDNN.CRDNN\n",
    "    input_shape: [null, null, !ref <n_mels>]\n",
    "    activation: !ref <activation>\n",
    "    dropout: !ref <dropout>\n",
    "    cnn_blocks: !ref <cnn_blocks>\n",
    "    cnn_channels: !ref <cnn_channels>\n",
    "    cnn_kernelsize: !ref <cnn_kernelsize>\n",
    "    inter_layer_pooling_size: !ref <inter_layer_pooling_size>\n",
    "    time_pooling: True\n",
    "    using_2d_pooling: False\n",
    "    time_pooling_size: !ref <time_pooling_size>\n",
    "    rnn_class: !ref <rnn_class>\n",
    "    rnn_layers: !ref <rnn_layers>\n",
    "    rnn_neurons: !ref <rnn_neurons>\n",
    "    rnn_bidirectional: !ref <rnn_bidirectional>\n",
    "    rnn_re_init: True\n",
    "    dnn_blocks: !ref <dnn_blocks>\n",
    "    dnn_neurons: !ref <dnn_neurons>\n",
    "    use_rnnp: False\n",
    "\n",
    "# Embedding (from indexes to an embedding space of dimension emb_size).\n",
    "embedding: !new:speechbrain.nnet.embedding.Embedding\n",
    "    num_embeddings: !ref <output_neurons>\n",
    "    embedding_dim: !ref <emb_size>\n",
    "\n",
    "# Attention-based RNN decoder.\n",
    "decoder: !new:speechbrain.nnet.RNN.AttentionalRNNDecoder\n",
    "    enc_dim: !ref <dnn_neurons>\n",
    "    input_size: !ref <emb_size>\n",
    "    rnn_type: gru\n",
    "    attn_type: location\n",
    "    hidden_size: !ref <dec_neurons>\n",
    "    attn_dim: 1024\n",
    "    num_layers: 1\n",
    "    scaling: 1.0\n",
    "    channels: 10\n",
    "    kernel_size: 100\n",
    "    re_init: True\n",
    "    dropout: !ref <dropout>\n",
    "\n",
    "# Linear transformation on the top of the encoder.\n",
    "ctc_lin: !new:speechbrain.nnet.linear.Linear\n",
    "    input_size: !ref <dnn_neurons>\n",
    "    n_neurons: !ref <output_neurons>\n",
    "\n",
    "# Linear transformation on the top of the decoder.\n",
    "seq_lin: !new:speechbrain.nnet.linear.Linear\n",
    "    input_size: !ref <dec_neurons>\n",
    "    n_neurons: !ref <output_neurons>\n",
    "\n",
    "# Final softmax (for log posteriors computation).\n",
    "log_softmax: !new:speechbrain.nnet.activations.Softmax\n",
    "    apply_log: True\n",
    "\n",
    "# Cost definition for the CTC part.\n",
    "ctc_cost: !name:speechbrain.nnet.losses.ctc_loss\n",
    "    blank_index: !ref <blank_index>\n",
    "\n",
    "# Tokenizer initialization\n",
    "tokenizer: !new:sentencepiece.SentencePieceProcessor\n",
    "\n",
    "# Objects in \"modules\" dict will have their parameters moved to the correct\n",
    "# device, as well as having train()/eval() called on them by the Brain class\n",
    "modules:\n",
    "    encoder: !ref <encoder>\n",
    "    embedding: !ref <embedding>\n",
    "    decoder: !ref <decoder>\n",
    "    ctc_lin: !ref <ctc_lin>\n",
    "    seq_lin: !ref <seq_lin>\n",
    "    normalize: !ref <normalize>\n",
    "    env_corrupt: !ref <env_corrupt>\n",
    "    lm_model: !ref <lm_model>\n",
    "\n",
    "# Gathering all the submodels in a single model object.\n",
    "model: !new:torch.nn.ModuleList\n",
    "    - - !ref <encoder>\n",
    "      - !ref <embedding>\n",
    "      - !ref <decoder>\n",
    "      - !ref <ctc_lin>\n",
    "      - !ref <seq_lin>\n",
    "\n",
    "# This is the RNNLM that is used according to the Huggingface repository\n",
    "# NB: It has to match the pre-trained RNNLM!!\n",
    "lm_model: !new:speechbrain.lobes.models.RNNLM.RNNLM\n",
    "    output_neurons: !ref <output_neurons>\n",
    "    embedding_dim: !ref <emb_size>\n",
    "    activation: !name:torch.nn.LeakyReLU\n",
    "    dropout: 0.0\n",
    "    rnn_layers: 2\n",
    "    rnn_neurons: 2048\n",
    "    dnn_blocks: 1\n",
    "    dnn_neurons: 512\n",
    "    return_hidden: True  # For inference\n",
    "```\n",
    "\n",
    "For instance, we define the function for computing features and normalizing them. We define the class for environmental corruption and data augmentation ([please, see this tutorial](https://colab.research.google.com/drive/1mAimqZndq0BwQj63VcDTr6_uCMC6i6Un?usp=sharing)), the architecture of the encoder, decoder, and the other models need by the speech recognizer.\n",
    "\n",
    "\n",
    "We then report the parameters for beasearch:\n",
    "\n",
    "```yaml\n",
    "valid_search: !new:speechbrain.decoders.S2SRNNBeamSearcher\n",
    "    embedding: !ref <embedding>\n",
    "    decoder: !ref <decoder>\n",
    "    linear: !ref <seq_lin>\n",
    "    ctc_linear: !ref <ctc_lin>\n",
    "    bos_index: !ref <bos_index>\n",
    "    eos_index: !ref <eos_index>\n",
    "    blank_index: !ref <blank_index>\n",
    "    min_decode_ratio: !ref <min_decode_ratio>\n",
    "    max_decode_ratio: !ref <max_decode_ratio>\n",
    "    beam_size: !ref <valid_beam_size>\n",
    "    eos_threshold: !ref <eos_threshold>\n",
    "    using_max_attn_shift: !ref <using_max_attn_shift>\n",
    "    max_attn_shift: !ref <max_attn_shift>\n",
    "    coverage_penalty: !ref <coverage_penalty>\n",
    "    temperature: !ref <temperature>\n",
    "\n",
    "# The final decoding on the test set can be more computationally demanding.\n",
    "# In this case, we use the LM + CTC probabilities during decoding as well.\n",
    "# Please, remove this part if you need a faster decoder.\n",
    "test_search: !new:speechbrain.decoders.S2SRNNBeamSearchLM\n",
    "    embedding: !ref <embedding>\n",
    "    decoder: !ref <decoder>\n",
    "    linear: !ref <seq_lin>\n",
    "    ctc_linear: !ref <ctc_lin>\n",
    "    language_model: !ref <lm_model>\n",
    "    bos_index: !ref <bos_index>\n",
    "    eos_index: !ref <eos_index>\n",
    "    blank_index: !ref <blank_index>\n",
    "    min_decode_ratio: !ref <min_decode_ratio>\n",
    "    max_decode_ratio: !ref <max_decode_ratio>\n",
    "    beam_size: !ref <test_beam_size>\n",
    "    eos_threshold: !ref <eos_threshold>\n",
    "    using_max_attn_shift: !ref <using_max_attn_shift>\n",
    "    max_attn_shift: !ref <max_attn_shift>\n",
    "    coverage_penalty: !ref <coverage_penalty>\n",
    "    lm_weight: !ref <lm_weight>\n",
    "    ctc_weight: !ref <ctc_weight_decode>\n",
    "    temperature: !ref <temperature>\n",
    "    temperature_lm: !ref <temperature_lm>\n",
    "```\n",
    "We here employ different hyperparameters for validation and test beamsearch. In particular, a smaller beam size is used for the validation stage. The reason is that validation is done at the end of each epoch and should thus be done quickly. Evaluation, instead, is done only once at the end and we can be more accurate.\n",
    "\n",
    "\n",
    "Finally, we declare the last objects needed by the training recipes, such as  lr_annealing, optimizer, checkpointer, etc:\n",
    "\n",
    "\n",
    "```yaml\n",
    "lr_annealing: !new:speechbrain.nnet.schedulers.NewBobScheduler\n",
    "    initial_value: !ref <lr>\n",
    "    improvement_threshold: 0.0025\n",
    "    annealing_factor: 0.8\n",
    "    patient: 0\n",
    "\n",
    "# This optimizer will be constructed by the Brain class after all parameters\n",
    "# are moved to the correct device. Then it will be added to the checkpointer.\n",
    "opt_class: !name:torch.optim.Adadelta\n",
    "    lr: !ref <lr>\n",
    "    rho: 0.95\n",
    "    eps: 1.e-8\n",
    "\n",
    "# Functions that compute the statistics to track during the validation step.\n",
    "error_rate_computer: !name:speechbrain.utils.metric_stats.ErrorRateStats\n",
    "\n",
    "cer_computer: !name:speechbrain.utils.metric_stats.ErrorRateStats\n",
    "    split_tokens: True\n",
    "\n",
    "# This object is used for saving the state of training both so that it\n",
    "# can be resumed if it gets interrupted, and also so that the best checkpoint\n",
    "# can be later loaded for evaluation or inference.\n",
    "checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n",
    "    checkpoints_dir: !ref <save_folder>\n",
    "    recoverables:\n",
    "        model: !ref <model>\n",
    "        scheduler: !ref <lr_annealing>\n",
    "        normalizer: !ref <normalize>\n",
    "        counter: !ref <epoch_counter>\n",
    "\n",
    "# This object is used to pretrain the language model and the tokenizers\n",
    "# (defined above). In this case, we also pretrain the ASR model (to make\n",
    "# sure the model converges on a small amount of data)\n",
    "pretrainer: !new:speechbrain.utils.parameter_transfer.Pretrainer\n",
    "    collect_in: !ref <save_folder>\n",
    "    loadables:\n",
    "        lm: !ref <lm_model>\n",
    "        tokenizer: !ref <tokenizer>\n",
    "        model: !ref <model>\n",
    "    paths:\n",
    "        lm: !ref <pretrained_path>/lm.ckpt\n",
    "        tokenizer: !ref <pretrained_path>/tokenizer.ckpt\n",
    "        model: !ref <pretrained_path>/asr.ckpt\n",
    "\n",
    "```\n",
    "\n",
    "The final object is the pretrainer that links the language model, the tokenizer, and the acoustic speech recognition model with their corresponding files used for pre-training.  We here pre-train the acoustic model as well. One such a small dataset, it is very hard to make an end-to-end speech recognizer converging and we thus use another model to pre-trained it (you should skip this part when training on a larger dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6xcAJ4OlYZCh"
   },
   "source": [
    "### **Experiment file**\n",
    "Let's now see how the different elements declared in the yaml files are connected in the train.py.\n",
    "The training script closely follows the one already described for the language model. \n",
    "\n",
    "The `main` function starts with the implementation of basic functionalities such as parsing the command line, initializing the distributed data-parallel (needed for multiple GPU training), and reading the yaml file.\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Reading command line arguments\n",
    "    hparams_file, run_opts, overrides = sb.parse_arguments(sys.argv[1:])\n",
    "\n",
    "    # Initialize ddp (useful only for multi-GPU DDP training)\n",
    "    sb.utils.distributed.ddp_init_group(run_opts)\n",
    "\n",
    "    # Load hyperparameters file with command-line overrides\n",
    "    with open(hparams_file) as fin:\n",
    "        hparams = load_hyperpyyaml(fin, overrides)\n",
    "\n",
    "    # Create experiment directory\n",
    "    sb.create_experiment_directory(\n",
    "        experiment_directory=hparams[\"output_folder\"],\n",
    "        hyperparams_to_save=hparams_file,\n",
    "        overrides=overrides,\n",
    "    )\n",
    "\n",
    "    # Data preparation, to be run on only one process.\n",
    "    sb.utils.distributed.run_on_main(\n",
    "        prepare_mini_librispeech,\n",
    "        kwargs={\n",
    "            \"data_folder\": hparams[\"data_folder\"],\n",
    "            \"save_json_train\": hparams[\"train_annotation\"],\n",
    "            \"save_json_valid\": hparams[\"valid_annotation\"],\n",
    "            \"save_json_test\": hparams[\"test_annotation\"],\n",
    "        },\n",
    "    )\n",
    "```\n",
    "\n",
    "The yaml file is read with the `load_hyperpyyaml` function. After reading it,  we will have all the declared object initialized and available with the hparams dictionary along with the other functions and variables (e.g, `hparams['model']`, `hparams['test_search']`,`hparams['batch_size']`).\n",
    "\n",
    "After that, we run the data preparation that has the goal of creating the data manifest file (if not already available). This operation requires writing some files on a disk. For this reason, we have to use the `sb.utils.distributed.run_on_main` to make sure that this operation is executed by the main process only. This avoids possible conflicts when using multiple GPUs with DDP. For more info on multi-gpu training in Speechbrai, [please see this tutorial](https://colab.research.google.com/drive/13pBUacPiotw1IvyffvGZ-HrtBr9T6l15?usp=sharing).\n",
    "\n",
    "#### **Data-IO Pipeline**\n",
    "At this point, we can create the dataset object that we will use for training, validation, and test loops:\n",
    "\n",
    "```python\n",
    "    # We can now directly create the datasets for training, valid, and test\n",
    "    datasets = dataio_prepare(hparams)\n",
    "```\n",
    "\n",
    "This function allows users to fully customize the data reading pipeline. Let's take a closer look into it:\n",
    "\n",
    "```python\n",
    "def dataio_prepare(hparams):\n",
    "    \"\"\"This function prepares the datasets to be used in the brain class.\n",
    "    It also defines the data processing pipeline through user-defined functions.\n",
    "\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    hparams : dict\n",
    "        This dictionary is loaded from the `train.yaml` file, and it includes\n",
    "        all the hyperparameters needed for dataset construction and loading.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    datasets : dict\n",
    "        Dictionary containing \"train\", \"valid\", and \"test\" keys that correspond\n",
    "        to the DynamicItemDataset objects.\n",
    "    \"\"\"\n",
    "    # Define audio pipeline. In this case, we simply read the path contained\n",
    "    # in the variable wav with the audio reader.\n",
    "    @sb.utils.data_pipeline.takes(\"wav\")\n",
    "    @sb.utils.data_pipeline.provides(\"sig\")\n",
    "    def audio_pipeline(wav):\n",
    "        \"\"\"Load the audio signal. This is done on the CPU in the `collate_fn`.\"\"\"\n",
    "        sig = sb.dataio.dataio.read_audio(wav)\n",
    "        return sig\n",
    "\n",
    "    # Define text processing pipeline. We start from the raw text and then\n",
    "    # encode it using the tokenizer. The tokens with BOS are used for feeding\n",
    "    # decoder during training, the tokens with EOS for computing the cost function.\n",
    "    # The tokens without BOS or EOS is for computing CTC loss.\n",
    "    @sb.utils.data_pipeline.takes(\"words\")\n",
    "    @sb.utils.data_pipeline.provides(\n",
    "        \"words\", \"tokens_list\", \"tokens_bos\", \"tokens_eos\", \"tokens\"\n",
    "    )\n",
    "    def text_pipeline(words):\n",
    "        \"\"\"Processes the transcriptions to generate proper labels\"\"\"\n",
    "        yield words\n",
    "        tokens_list = hparams[\"tokenizer\"].encode_as_ids(words)\n",
    "        yield tokens_list\n",
    "        tokens_bos = torch.LongTensor([hparams[\"bos_index\"]] + (tokens_list))\n",
    "        yield tokens_bos\n",
    "        tokens_eos = torch.LongTensor(tokens_list + [hparams[\"eos_index\"]])\n",
    "        yield tokens_eos\n",
    "        tokens = torch.LongTensor(tokens_list)\n",
    "        yield tokens\n",
    "\n",
    "    # Define datasets from json data manifest file\n",
    "    # Define datasets sorted by ascending lengths for efficiency\n",
    "    datasets = {}\n",
    "    data_folder = hparams[\"data_folder\"]\n",
    "    for dataset in [\"train\", \"valid\", \"test\"]:\n",
    "        datasets[dataset] = sb.dataio.dataset.DynamicItemDataset.from_json(\n",
    "            json_path=hparams[f\"{dataset}_annotation\"],\n",
    "            replacements={\"data_root\": data_folder},\n",
    "            dynamic_items=[audio_pipeline, text_pipeline],\n",
    "            output_keys=[\n",
    "                \"id\",\n",
    "                \"sig\",\n",
    "                \"words\",\n",
    "                \"tokens_bos\",\n",
    "                \"tokens_eos\",\n",
    "                \"tokens\",\n",
    "            ],\n",
    "        )\n",
    "        hparams[f\"{dataset}_dataloader_opts\"][\"shuffle\"] = False\n",
    "\n",
    "    # Sorting traiing data with ascending order makes the code  much\n",
    "    # faster  because we minimize zero-padding. In most of the cases, this\n",
    "    # does not harm the performance.\n",
    "    if hparams[\"sorting\"] == \"ascending\":\n",
    "        datasets[\"train\"] = datasets[\"train\"].filtered_sorted(sort_key=\"length\")\n",
    "        hparams[\"train_dataloader_opts\"][\"shuffle\"] = False\n",
    "\n",
    "    elif hparams[\"sorting\"] == \"descending\":\n",
    "        datasets[\"train\"] = datasets[\"train\"].filtered_sorted(\n",
    "            sort_key=\"length\", reverse=True\n",
    "        )\n",
    "        hparams[\"train_dataloader_opts\"][\"shuffle\"] = False\n",
    "\n",
    "    elif hparams[\"sorting\"] == \"random\":\n",
    "        hparams[\"train_dataloader_opts\"][\"shuffle\"] = True\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            \"sorting must be random, ascending or descending\"\n",
    "        )\n",
    "    return datasets\n",
    "```\n",
    "\n",
    "Within `dataio_prepare` we define subfunctions for processing the entries defined in the JSON files. \n",
    "The first function, called `audio_pipeline` takes the path of the audio signal (`wav`) and reads it. It returns a tensor containing the read speech sentence. The entry in input to this function (i.e, `wav`) must have the same name of the corresponding key in the data manifest file:\n",
    "\n",
    "```json\n",
    "  \"1867-154075-0032\": {\n",
    "    \"wav\": \"{data_root}/LibriSpeech/train-clean-5/1867/154075/1867-154075-0032.flac\",\n",
    "    \"length\": 16.09,\n",
    "    \"words\": \"AND HE BRUSHED A HAND ACROSS HIS FOREHEAD AND WAS INSTANTLY HIMSELF CALM AND COOL VERY WELL THEN IT SEEMS I'VE MADE AN ASS OF MYSELF BUT I'LL TRY TO MAKE UP FOR IT NOW WHAT ABOUT CAROLINE\"\n",
    "  },\n",
    "```\n",
    "\n",
    "Similarly, we define another function called `text_pipeline` for processing the signal transcriptions and put them in a format usable by the defined model. The function reads the string `words` defined in the JSON file and tokenizes it (outputting the index of each token). It return the sequence of tokens with the special begin-of-sentence `<bos>` token in front, and the version with the end-of-sentence `<eos>` token at the end aswell. We will see later why these additional elements are needed.\n",
    "\n",
    "We then create the `DynamicItemDataset` and connect it with the processing functions defined above. We define the desired output keys. These keys will be available in the brain class within the batch variable as:\n",
    "- batch.id\n",
    "- batch.sig\n",
    "- batch.words\n",
    "- batch.tokens_bos\n",
    "- batch.tokens_eos\n",
    "- batch.tokens\n",
    "\n",
    "The last part of the `dataio_prepare` function manages data sorting. In this case, we sort data in ascending order to minimize zero paddings and speeding training up. For more information on the dataloaders, [please see this tutorial](https://colab.research.google.com/drive/1AiVJZhZKwEI4nFGANKXEe-ffZFfvXKwH?usp=sharing)\n",
    "\n",
    "\n",
    "After the definition of the dataio function, we perform pre-training of the language model, ASR model, and tokenizer:\n",
    "\n",
    "\n",
    "```python\n",
    "    run_on_main(hparams[\"pretrainer\"].collect_files)\n",
    "    hparams[\"pretrainer\"].load_collected(device=run_opts[\"device\"])\n",
    "```\n",
    "We here use the `run_on_main` wrapper because the ` collect_files` method might need to download the pre-trained model from the web. This operation should be done by a single process only even when using multiple GPUs with DDP).\n",
    "\n",
    "At this point we initialize the Brain class and use it for running training and evaluation:\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "    # Trainer initialization\n",
    "    asr_brain = ASR(\n",
    "        modules=hparams[\"modules\"],\n",
    "        opt_class=hparams[\"opt_class\"],\n",
    "        hparams=hparams,\n",
    "        run_opts=run_opts,\n",
    "        checkpointer=hparams[\"checkpointer\"],\n",
    "    )\n",
    "\n",
    "    # Training\n",
    "    asr_brain.fit(\n",
    "        asr_brain.hparams.epoch_counter,\n",
    "        datasets[\"train\"],\n",
    "        datasets[\"valid\"],\n",
    "        train_loader_kwargs=hparams[\"train_dataloader_opts\"],\n",
    "        valid_loader_kwargs=hparams[\"valid_dataloader_opts\"],\n",
    "    )\n",
    "\n",
    "    # Load best checkpoint for evaluation\n",
    "    test_stats = asr_brain.evaluate(\n",
    "        test_set=datasets[\"test\"],\n",
    "        min_key=\"WER\",\n",
    "        test_loader_kwargs=hparams[\"test_dataloader_opts\"],\n",
    "    )\n",
    "```\n",
    "\n",
    "For more information on how the Brain class works, [please see this tutorial](https://colab.research.google.com/drive/1fdqTk4CTXNcrcSVFvaOKzRfLmj4fJfwa?usp=sharing) \n",
    "Note that the `fit` and `evaluate` methods take in input the dataset objects as well. From this dataset, a pytorch dataloader is created automatically. The latter creates the batches used for training and evaluation. \n",
    "\n",
    "When speech sentences with **different lengths** are sampled, zero-padding is performed. To keep track of the real length of each sentence within each batch, the dataloader returns a special tensor containing **relative lengths** as well. For instance, let's assume `batch.sig[0]` to be variable that contains the input waveform as a [batch, time] tensor:\n",
    "\n",
    "```\n",
    "tensor([[1, 1, 0, 0],\n",
    "        [1, 1, 1, 0],\n",
    "        [1, 1, 0, 0]])\n",
    "```\n",
    "The `batch.sig[1]` will contain the following relative lengths:\n",
    "\n",
    "```\n",
    "tensor([0.5000, 0.7500, 1.0000])\n",
    "```\n",
    "\n",
    "With this information, we can exclude zero-padded steps from some computations (e.g feature normalization, statistical pooling, loss, etc). \n",
    "\n",
    "### Why relative lengths instead of absolute lengths?\n",
    "\n",
    "The reason is that the **time resolution can change** within a neural network. There are operations such as pooling, stride convolution, transposed convolution, FFT computation, and many others that change the number of time steps. With the relative position trick, we can compute the number of actual time steps in each stage of the neural computations just by multiplying the relative length by the length of the tensor.\n",
    "\n",
    "\n",
    "#### **Forward Computations**\n",
    "In the Brain class we have to define some important methods such as:\n",
    "- `compute_forward`, that specifies all the computations needed to transform the input waveform into the output posterior probabilities)\n",
    "- `compute_objective`, which computes the loss function given the labels and the predictions performed by the model.\n",
    "\n",
    "Let's take a look into `compute_forward` first:\n",
    "\n",
    "\n",
    "```python\n",
    "    def compute_forward(self, batch, stage):\n",
    "        \"\"\"Runs all the computation of the CTC + seq2seq ASR. It returns the\n",
    "        posterior probabilities of the CTC and seq2seq networks.\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        batch : PaddedBatch\n",
    "            This batch object contains all the relevant tensors for computation.\n",
    "        stage : sb.Stage\n",
    "            One of sb.Stage.TRAIN, sb.Stage.VALID, or sb.Stage.TEST.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        predictions : dict\n",
    "            At training time it returns predicted seq2seq log probabilities.\n",
    "            If needed it also returns the ctc output log probabilities.\n",
    "            At validation/test time, it returns the predicted tokens as well.\n",
    "        \"\"\"\n",
    "        # We first move the batch to the appropriate device.\n",
    "        batch = batch.to(self.device)\n",
    "        feats, self.feat_lens = self.prepare_features(stage, batch.sig)\n",
    "        tokens_bos, _ = self.prepare_tokens(stage, batch.tokens_bos)\n",
    "\n",
    "        # Running the encoder (prevent propagation to feature extraction)\n",
    "        encoded_signal = self.modules.encoder(feats.detach())\n",
    "\n",
    "        # Embed tokens and pass tokens & encoded signal to decoder\n",
    "        embedded_tokens = self.modules.embedding(tokens_bos)\n",
    "        decoder_outputs, _ = self.modules.decoder(\n",
    "            embedded_tokens, encoded_signal, self.feat_lens\n",
    "        )\n",
    "\n",
    "        # Output layer for seq2seq log-probabilities\n",
    "        logits = self.modules.seq_lin(decoder_outputs)\n",
    "        predictions = {\"seq_logprobs\": self.hparams.log_softmax(logits)}\n",
    "\n",
    "        if self.is_ctc_active(stage):\n",
    "            # Output layer for ctc log-probabilities\n",
    "            ctc_logits = self.modules.ctc_lin(encoded_signal)\n",
    "            predictions[\"ctc_logprobs\"] = self.hparams.log_softmax(ctc_logits)\n",
    "        elif stage == sb.Stage.VALID:\n",
    "            predictions[\"tokens\"], _ = self.hparams.valid_search(\n",
    "                encoded_signal, self.feat_lens\n",
    "            )\n",
    "        elif stage == sb.Stage.TEST:\n",
    "            predictions[\"tokens\"], _ = self.hparams.test_search(\n",
    "                encoded_signal, self.feat_lens\n",
    "            )\n",
    "\n",
    "        return predictions\n",
    "```\n",
    "\n",
    "\n",
    "The function takes the batch variable and the current stage (that can be `sb.Stage.TRAIN`, `sb.Stage.VALID`, or `sb.Stage.TEST`). We then put the batch on the right device, compute the features, and encode them with our CRDNN encoder. \n",
    "For more information on feature computation, [take a look into this tutorial](https://colab.research.google.com/drive/1CI72Xyay80mmmagfLaIIeRoDgswWHT_g?usp=sharing), while for more details on the speech augmentation [take a look here](https://colab.research.google.com/drive/1JJc4tBhHNXRSDM2xbQ3Z0jdDQUw4S5lr?usp=sharing).\n",
    "After that, we feed our encoded states into an autoregressive attention-based decoder that performs some predictions over the tokens.\n",
    "At validation and test stages, we apply beamsearch on the top of the token predictions. \n",
    "Our system applies an additional CTC loss on the top of the encoder. The CTC can be turned off after N epochs if desired.\n",
    "\n",
    "\n",
    "#### **Compute Objectives**\n",
    "\n",
    "Let's take a look now into the compute_objectives function:\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    " def compute_objectives(self, predictions, batch, stage):\n",
    "        \"\"\"Computes the loss given the predicted and targeted outputs. We here\n",
    "        do multi-task learning and the loss is a weighted sum of the ctc + seq2seq\n",
    "        costs.\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        predictions : dict\n",
    "            The output dict from `compute_forward`.\n",
    "        batch : PaddedBatch\n",
    "            This batch object contains all the relevant tensors for computation.\n",
    "        stage : sb.Stage\n",
    "            One of sb.Stage.TRAIN, sb.Stage.VALID, or sb.Stage.TEST.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss : torch.Tensor\n",
    "            A one-element tensor used for backpropagating the gradient.\n",
    "        \"\"\"\n",
    "        # Compute sequence loss against targets with EOS\n",
    "        tokens_eos, tokens_eos_lens = self.prepare_tokens(\n",
    "            stage, batch.tokens_eos\n",
    "        )\n",
    "        loss = sb.nnet.losses.nll_loss(\n",
    "            log_probabilities=predictions[\"seq_logprobs\"],\n",
    "            targets=tokens_eos,\n",
    "            length=tokens_eos_lens,\n",
    "            label_smoothing=self.hparams.label_smoothing,\n",
    "        )\n",
    "\n",
    "        # Add ctc loss if necessary. The total cost is a weighted sum of\n",
    "        # ctc loss + seq2seq loss\n",
    "        if self.is_ctc_active(stage):\n",
    "            # Load tokens without EOS as CTC targets\n",
    "            tokens, tokens_lens = self.prepare_tokens(stage, batch.tokens)\n",
    "            loss_ctc = self.hparams.ctc_cost(\n",
    "                predictions[\"ctc_logprobs\"], tokens, self.feat_lens, tokens_lens\n",
    "            )\n",
    "            loss *= 1 - self.hparams.ctc_weight\n",
    "            loss += self.hparams.ctc_weight * loss_ctc\n",
    "\n",
    "        if stage != sb.Stage.TRAIN:\n",
    "            # Converted predicted tokens from indexes to words\n",
    "            predicted_words = [\n",
    "                self.hparams.tokenizer.decode_ids(prediction).split(\" \")\n",
    "                for prediction in predictions[\"tokens\"]\n",
    "            ]\n",
    "            target_words = [words.split(\" \") for words in batch.words]\n",
    "\n",
    "            # Monitor word error rate and character error rated at\n",
    "            # valid and test time.\n",
    "            self.wer_metric.append(batch.id, predicted_words, target_words)\n",
    "            self.cer_metric.append(batch.id, predicted_words, target_words)\n",
    "\n",
    "        return loss\n",
    "```\n",
    "\n",
    "Based on the predictions and the target we compute the Negative Log Likelihood  loss (NLL) and, if needed, the Connectionist Temporal Classification (CTC) one as well. The two losses are combined with a weight (ctc_weight). At validation or test stages,  we compute the word-error-rate (WER) and the character-error-rate (CER). \n",
    "\n",
    "### **Other Methods**\n",
    "Beyond `forward and `compute_objective` you can find other functions such as `on_stage_start` and `on_stage_end`. The first just initializes the  statistic objects (e.g, WER and CER), while the second manages:\n",
    "- statistics updates\n",
    "- learning rate annealing\n",
    "- logging\n",
    "- checkpointing\n",
    "\n",
    "That's all. You can just run the code and train your speech recognizer.\n",
    "\n",
    "\n",
    "The current code implements all the needed functionalities to train a state-of-the-art speech recognition system.  In a real case, however, you have to train the model with a much larger dataset to reach acceptable performance. As an example, [you can see our LibriSpeech recipes here](https://github.com/speechbrain/speechbrain/tree/develop/recipes/LibriSpeech/ASR). For more information on checkpointing, [take a look here](https://colab.research.google.com/drive/1VH7U0oP3CZsUNtChJT2ewbV_q1QX8xre?usp=sharing).\n",
    "\n",
    "## **Pretrain and Fine-tune**\n",
    "In some cases, instead of training the mode from scratch you might wanna start from a pre-trained model and fine-tune it. Note that to make it possible, the architecure of your model must match exactly with the pre-trained one. \n",
    "\n",
    "One convenient way, is to use the pretrain class in the yaml file. If you want to pretrain the encoder of the speech recognizer, you can use the following code: \n",
    "\n",
    "```yaml\n",
    "pretrainer: !new:speechbrain.utils.parameter_transfer.Pretrainer\n",
    " loadables:\n",
    "     encoder: !ref <encoder>\n",
    " paths:\n",
    "   encoder: !ref <encoder_ptfile>\n",
    "```\n",
    "\n",
    "where `!ref <encoder>` points the the encoder model previously define in the yaml file, and `encoder_ptfile` is the path where you have stored your pre-train model.\n",
    "\n",
    "To perform pre-training, make sure to call the pre-trained in the `train.py`:\n",
    "\n",
    "```\n",
    "run_on_main(hparams[\"pretrainer\"].collect_files)\n",
    "    hparams[\"pretrainer\"].load_collected(device=run_opts[\"device\"])\n",
    "```\n",
    "You have to call this function before the fit method of the brain class.\n",
    "\n",
    "For more information, [please take a look into our tutorial on pre-training and fine-tune](https://colab.research.google.com/drive/1LN7R3U3xneDgDRK2gC5MzGkLysCWxuC3?usp=sharing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4LnRq1_cpPXZ"
   },
   "source": [
    "## **Step 5: Inference**\n",
    "\n",
    "At this point, we can use the trained speech recognizer. For this type of ASR model, speechbrain made available some classes ([take a look here](https://github.com/speechbrain/speechbrain/blob/develop/speechbrain/pretrained/interfaces.py)) such as the `EncoderDecoderASR` one that can make inference easier. For instance, we can transcribe an audio file with a pre-trained model hosted in our [HuggingFace repository](https://huggingface.co/speechbrain) in solely 4 lines of code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197,
     "referenced_widgets": [
      "793ef14b7b004096981c55e7bf4fbdae",
      "c8edc1eae8084ade9b3e02be21ac6b36",
      "ba5f07c308fa42f0963848266d7e376d",
      "742aaf25ce0b4911ad1b17aced2a411f",
      "5a908cae48844f8ca14baf189d7e58bd",
      "676c443474a64391bb0398cfd3b7d7c5",
      "8d8f39950d0d416395d3c1c2afc03396",
      "1d871e41a1c346c0941ff8bf286a8fc2",
      "2b38891bfbf148c99c8f770a46c129dd",
      "772373f931bc42869027b4db253737ad",
      "1060fe2ec0774468a5903acb2b7f9a87",
      "6248b94de0464c2b933add8e4f6b7af4",
      "48901abf068b4242a014050db20b0631",
      "eff25baa67674ee58e9715e78cd5b7a1",
      "f917b17d56734fb1b8cb471e51b15328",
      "2a8a07ff265941c78d3dd4915f5c11a1",
      "31a565eb19ab4091a965a1191a0f9e65",
      "1b9fd09c88184811ad52d2b03c60422d",
      "32f78deb548546a788a92d9507f6eb72",
      "e239d2cbb12346ffb706cacab4dca38a",
      "209d0ef110b343d08759c8daeaeb78b8",
      "19800374b3cc44db9c82637d3981265a",
      "16fd875c6034418d844de3aa5f18d8d9",
      "1247b9b12f884d00b318569af5d1c33f",
      "2569d3526a6742828af2d9360a67d312",
      "472f38e4f2514fcca5840a19bd6b80c7",
      "cf48aa6b98014f97a974ca398287b6a4",
      "07aadeb0ed4c4fc7a9ea91b3938a2e02",
      "afa3cd14810244bf92d8814bd695f3b6",
      "69bcb272473c45caac52ac93725ab4e1",
      "cb36b99b7ee24cefa46590ecc0122b72",
      "26573bd508184e5faac952104b8784b5",
      "2e7f8ea4c64d49478573974925975f18",
      "2b7c662594354cfa9327ba21f93fe51e",
      "b5ad90b6a6924ae3993ff2a9751c917c",
      "3af10f6cc57a47719bfca37ee59eb84f",
      "b176842c64324374b0d42325dbdae328",
      "55e848a9b4264094b5e9fa9e29adf605",
      "326aecdc3b3342ceb3d0bb82c1b6ca93",
      "855230c0c2fd46fcb28a63065ce5ebd7",
      "cc771ea227a94bcf9ffda70f5085f90f",
      "81573628e9ef4ec18a325a0d62b790e8",
      "c5cd1cec1745490aa00041eaccf3f4e3",
      "03842889ca4f42ddabecf025fa71cfd1",
      "9e8b3e216161422bb18c494ee3da756d",
      "4377bcef40f142efaa448b925d36730f",
      "2e96792c1c3949ef9deb839df5c3f8e9",
      "20d770de4aa74489a9f816cf038809b4",
      "34181d8ec5fa45fe858b56c3eb86aed6",
      "f207a275c2624be9b89d74b2f0fe2738",
      "418580459dc54712a947fbdf6ec8bd37",
      "d7887ea69bfe4ffa931d1a730d0e9e02",
      "e3fecdd0618745928f6d778aebc05afa",
      "5ccec8b8ad064037bf7f55ebb9e23907",
      "3f9828cfafc14ee9935039fd43f4c6a0"
     ]
    },
    "id": "uvvY0dCbx5Sv",
    "outputId": "bd7f3294-25f4-486a-de35-acdb5d0382c4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "793ef14b7b004096981c55e7bf4fbdae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/4.42k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6248b94de0464c2b933add8e4f6b7af4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/480M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16fd875c6034418d844de3aa5f18d8d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/212M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b7c662594354cfa9327ba21f93fe51e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/253k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e8b3e216161422bb18c494ee3da756d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/104k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'THE BIRCH CANOE SLID ON THE SMOOTH PLANKS'"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from speechbrain.pretrained import EncoderDecoderASR\n",
    "\n",
    "asr_model = EncoderDecoderASR.from_hparams(source=\"speechbrain/asr-crdnn-rnnlm-librispeech\", savedir=\"pretrained_model\")\n",
    "audio_file = 'speechbrain/asr-crdnn-rnnlm-librispeech/example.wav'\n",
    "asr_model.transcribe_file(audio_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Dyv9x10gGzV"
   },
   "source": [
    "But, how does this work with your custom ASR system? \n",
    "\n",
    "### **Train and use your speech recognizer on your data**\n",
    "\n",
    "At this point, three options are available to you:\n",
    "1. Define a custom python function in your ASR class (extended from Brain). This introduces strong coupling between the training recipe and your transcripts. It is pretty convenient for prototyping and obtaining simple transcripts on your datasets. However, it is not recommended for deployment. \n",
    "2. Use already available Interfaces (such as `EncoderDecoderASR`). This is probably the most elegant and convenient way. However, your model should be compliant with some constraints to fit the proposed interface.\n",
    "3. Build your own Interface perfectly fitting to your custom ASR model.\n",
    "\n",
    "**Important: All these solutions also apply for other tasks (speaker recognition, source separation ...)**\n",
    "\n",
    "#### **Custom function in the training script**\n",
    "The goal of this approach is to enable the user to call a function at the end of `train.py` that transcribes a given dataset:\n",
    "\n",
    "```python\n",
    "# Trainer initialization\n",
    "    asr_brain = ASR(\n",
    "        modules=hparams[\"modules\"],\n",
    "        opt_class=hparams[\"opt_class\"],\n",
    "        hparams=hparams,\n",
    "        run_opts=run_opts,\n",
    "        checkpointer=hparams[\"checkpointer\"],\n",
    "    )\n",
    " \n",
    "    # Training\n",
    "    asr_brain.fit(\n",
    "        asr_brain.hparams.epoch_counter,\n",
    "        datasets[\"train\"],\n",
    "        datasets[\"valid\"],\n",
    "        train_loader_kwargs=hparams[\"train_dataloader_opts\"],\n",
    "        valid_loader_kwargs=hparams[\"valid_dataloader_opts\"],\n",
    "    )\n",
    " \n",
    "    # Load best checkpoint for evaluation\n",
    "    test_stats = asr_brain.evaluate(\n",
    "        test_set=datasets[\"test\"],\n",
    "        min_key=\"WER\",\n",
    "        test_loader_kwargs=hparams[\"test_dataloader_opts\"],\n",
    "    )\n",
    "\n",
    "    # Load best checkpoint for transcription !!!!!!\n",
    "    # You need to create this function w.r.t your system architecture !!!!!!\n",
    "    transcripts = asr_brain.transcribe_dataset(\n",
    "        dataset=datasets[\"your_dataset\"], # Must be obtained from the dataio_function\n",
    "        min_key=\"WER\", # We load the model with the lowest WER\n",
    "        loader_kwargs=hparams[\"transcribe_dataloader_opts\"], # opts for the dataloading\n",
    "    )\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eW7XjIuig-Tx"
   },
   "source": [
    "As you can see, there exists a strong coupling with the training recipe due to the need for an instantiated Brain class. \n",
    "\n",
    "**Note 1:** You can remove the `.fit()` and `.evaluate()` if you don't want to call them. This is just an example to better highlight how to use it.\n",
    "\n",
    "**Note 2:** Here, the `.transcribe_dataset()` function takes a `dataset` object to transcribe. You could also simply use a path instead. It is **completely** up to you to implement this function as you wish. \n",
    "\n",
    "Now: what to put in this function? Here, we will give an example based on the template, but you will need to adapt it to **your** system.\n",
    "\n",
    "```python\n",
    "\n",
    "def transcribe_dataset(\n",
    "        self,\n",
    "        dataset, # Must be obtained from the dataio_function\n",
    "        min_key, # We load the model with the lowest WER\n",
    "        loader_kwargs # opts for the dataloading\n",
    "    ):\n",
    "  \n",
    "    # If dataset isn't a Dataloader, we create it. \n",
    "    if not isinstance(dataset, DataLoader):\n",
    "        loader_kwargs[\"ckpt_prefix\"] = None\n",
    "        dataset = self.make_dataloader(\n",
    "            dataset, Stage.TEST, **loader_kwargs\n",
    "        )\n",
    "    \n",
    "    \n",
    "    self.on_evaluate_start(min_key=min_key) # We call the on_evaluate_start that will load the best model\n",
    "    self.modules.eval() # We set the model to eval mode (remove dropout etc)\n",
    "\n",
    "    # Now we iterate over the dataset and we simply compute_forward and decode\n",
    "    with torch.no_grad():\n",
    "\n",
    "        transcripts = []\n",
    "        for batch in tqdm(dataset, dynamic_ncols=True):\n",
    "            \n",
    "            # Make sure that your compute_forward returns the predictions !!!\n",
    "            # In the case of the template, when stage = TEST, a beam search is applied \n",
    "            # in compute_forward(). \n",
    "            out = self.compute_forward(batch, stage=sb.Stage.TEST) \n",
    "            p_seq, wav_lens, predicted_tokens = out\n",
    "            \n",
    "            # We go from tokens to words.\n",
    "            predicted_words = self.tokenizer(\n",
    "                predicted_tokens, task=\"decode_from_list\"\n",
    "            )\n",
    "            transcripts.append(predicted_words)\n",
    "            \n",
    "    return transcripts\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J6N0Fb51pFnZ"
   },
   "source": [
    "The pipeline is simple: load the model -> do compute_forward -> detokenize.\n",
    "\n",
    "#### **Using the `EndoderDecoderASR` interface**\n",
    "\n",
    "The [EncoderDecoderASR class](https://github.com/speechbrain/speechbrain/blob/develop/speechbrain/pretrained/interfaces.py#L353). interface allows you to decouple your trained model from the training recipe and to infer (or encode) on any new audio file in few lines of code. The class has the following methods:\n",
    "\n",
    "- *encode_batch*: apply the encoder to an input batch and returns some encoded features.\n",
    "- *transcribe_file*: transcribes the single audio file in input.\n",
    "- *transcribe_batch*: transcribes the input batch.\n",
    "\n",
    "In fact, if you fulfill few constraints that we will detail in the next paragraph, you can simply do:\n",
    "\n",
    "```python\n",
    "from speechbrain.pretrained import EncoderDecoderASR\n",
    "\n",
    "asr_model = EncoderDecoderASR.from_hparams(source=\"your_local_folder\", hparams_file='your_file.yaml', savedir=\"pretrained_model\")\n",
    "audio_file = 'your_file.wav'\n",
    "asr_model.transcribe_file(audio_file)\n",
    "```\n",
    "\n",
    "Nevertheless, to allow such a generalization over all the possible EncoderDecoder ASR pipelines, you will have to consider a few constraints when deploying your system:\n",
    "\n",
    "1. **Necessary modules.** As you can see in the `EncoderDecoderASR` class, the modules defined in your yaml file MUST contain certain elements with specific names. In practice, you need a tokenizer, a decoder, and a decoder. The encoder can simply be a `speechbrain.nnet.containers.LengthsCapableSequential` composed with a sequence of features computation, normalization and model encoding. \n",
    "```python\n",
    "    HPARAMS_NEEDED = [\"tokenizer\"]\n",
    "    MODULES_NEEDED = [\n",
    "        \"encoder\",\n",
    "        \"decoder\",\n",
    "    ]\n",
    "```\n",
    "\n",
    "You also need to declare these entities in the YAML file and create the following dictionary called `modules`:\n",
    "\n",
    "```\n",
    "encoder: !new:speechbrain.nnet.containers.LengthsCapableSequential\n",
    "    input_shape: [null, null, !ref <n_mels>]\n",
    "    compute_features: !ref <compute_features>\n",
    "    normalize: !ref <normalize>\n",
    "    model: !ref <enc>\n",
    "\n",
    "decoder: !new:speechbrain.decoders.S2SRNNBeamSearchLM\n",
    "    embedding: !ref <emb>\n",
    "    decoder: !ref <dec>\n",
    "    linear: !ref <seq_lin>\n",
    "    language_model: !ref <lm_model>\n",
    "    bos_index: !ref <bos_index>\n",
    "    eos_index: !ref <eos_index>\n",
    "    min_decode_ratio: !ref <min_decode_ratio>\n",
    "    max_decode_ratio: !ref <max_decode_ratio>\n",
    "    beam_size: !ref <beam_size>\n",
    "    eos_threshold: !ref <eos_threshold>\n",
    "    using_max_attn_shift: !ref <using_max_attn_shift>\n",
    "    max_attn_shift: !ref <max_attn_shift>\n",
    "    coverage_penalty: !ref <coverage_penalty>\n",
    "    lm_weight: !ref <lm_weight>\n",
    "    temperature: !ref <temperature>\n",
    "    temperature_lm: !ref <temperature_lm>\n",
    "\n",
    "modules:\n",
    "    encoder: !ref <encoder>\n",
    "    decoder: !ref <decoder>\n",
    "    lm_model: !ref <lm_model>\n",
    "```\n",
    "\n",
    "In this case, `enc` is a CRDNN, but could be any custom neural network for instance.\n",
    "\n",
    "  **Why do you need to ensure this?** Well, it simply is because these are the modules we call when inferring on the `EncoderDecoderASR` class. Here is an example of the `encode_batch()` function.\n",
    "```python\n",
    "[...]\n",
    "  wavs = wavs.float()\n",
    "  wavs, wav_lens = wavs.to(self.device), wav_lens.to(self.device)\n",
    "  encoder_out = self.modules.encoder(wavs, wav_lens)\n",
    "return encoder_out\n",
    "```\n",
    "  **What if I have a complex asr_encoder structure with multiple deep neural networks and stuffs ?** Simply put everything in a torch.nn.ModuleList in your yaml:\n",
    "```yaml\n",
    "asr_encoder: !new:torch.nn.ModuleList\n",
    "    - [!ref <enc>, my_different_blocks ... ]\n",
    "```\n",
    "\n",
    "2. **Call to the pretrainer to load the checkpoints.** Finally, you need to define a call to the pretrainer that will load the different checkpoints of your trained model into the corresponding SpeechBrain modules. In short, it will load the weights of your encoder, language model or even simply load the tokenizer. \n",
    "```yaml\n",
    "pretrainer: !new:speechbrain.utils.parameter_transfer.Pretrainer\n",
    "    loadables:\n",
    "        asr: !ref <asr_model>\n",
    "        lm: !ref <lm_model>\n",
    "        tokenizer: !ref <tokenizer>\n",
    "    paths:\n",
    "      asr: !ref <asr_model_ptfile>\n",
    "      lm: !ref <lm_model_ptfile>\n",
    "      tokenizer: !ref <tokenizer_ptfile>\n",
    "```\n",
    "The loadable field creates a link between a file (e.g. `lm` that is related to the checkpoint in `<lm_model_ptfile>`) to a yaml instance (e.g. `<lm_model>`) that is nothing more than your lm. \n",
    "\n",
    "If you respect these two constraints, it should works! Here, we give a complete example of a yaml that is used for inference only:\n",
    "\n",
    "```yaml\n",
    "\n",
    "# ############################################################################\n",
    "# Model: E2E ASR with attention-based ASR\n",
    "# Encoder: CRDNN model\n",
    "# Decoder: GRU + beamsearch + RNNLM\n",
    "# Tokens: BPE with unigram\n",
    "# Authors:  Ju-Chieh Chou, Mirco Ravanelli, Abdel Heba, Peter Plantinga 2020\n",
    "# ############################################################################\n",
    "\n",
    "\n",
    "# Feature parameters\n",
    "sample_rate: 16000\n",
    "n_fft: 400\n",
    "n_mels: 40\n",
    "\n",
    "# Model parameters\n",
    "activation: !name:torch.nn.LeakyReLU\n",
    "dropout: 0.15\n",
    "cnn_blocks: 2\n",
    "cnn_channels: (128, 256)\n",
    "inter_layer_pooling_size: (2, 2)\n",
    "cnn_kernelsize: (3, 3)\n",
    "time_pooling_size: 4\n",
    "rnn_class: !name:speechbrain.nnet.RNN.LSTM\n",
    "rnn_layers: 4\n",
    "rnn_neurons: 1024\n",
    "rnn_bidirectional: True\n",
    "dnn_blocks: 2\n",
    "dnn_neurons: 512\n",
    "emb_size: 128\n",
    "dec_neurons: 1024\n",
    "output_neurons: 1000  # index(blank/eos/bos) = 0\n",
    "blank_index: 0\n",
    "\n",
    "# Decoding parameters\n",
    "bos_index: 0\n",
    "eos_index: 0\n",
    "min_decode_ratio: 0.0\n",
    "max_decode_ratio: 1.0\n",
    "beam_size: 80\n",
    "eos_threshold: 1.5\n",
    "using_max_attn_shift: True\n",
    "max_attn_shift: 240\n",
    "lm_weight: 0.50\n",
    "coverage_penalty: 1.5\n",
    "temperature: 1.25\n",
    "temperature_lm: 1.25\n",
    "\n",
    "normalize: !new:speechbrain.processing.features.InputNormalization\n",
    "    norm_type: global\n",
    "\n",
    "compute_features: !new:speechbrain.lobes.features.Fbank\n",
    "    sample_rate: !ref <sample_rate>\n",
    "    n_fft: !ref <n_fft>\n",
    "    n_mels: !ref <n_mels>\n",
    "\n",
    "enc: !new:speechbrain.lobes.models.CRDNN.CRDNN\n",
    "    input_shape: [null, null, !ref <n_mels>]\n",
    "    activation: !ref <activation>\n",
    "    dropout: !ref <dropout>\n",
    "    cnn_blocks: !ref <cnn_blocks>\n",
    "    cnn_channels: !ref <cnn_channels>\n",
    "    cnn_kernelsize: !ref <cnn_kernelsize>\n",
    "    inter_layer_pooling_size: !ref <inter_layer_pooling_size>\n",
    "    time_pooling: True\n",
    "    using_2d_pooling: False\n",
    "    time_pooling_size: !ref <time_pooling_size>\n",
    "    rnn_class: !ref <rnn_class>\n",
    "    rnn_layers: !ref <rnn_layers>\n",
    "    rnn_neurons: !ref <rnn_neurons>\n",
    "    rnn_bidirectional: !ref <rnn_bidirectional>\n",
    "    rnn_re_init: True\n",
    "    dnn_blocks: !ref <dnn_blocks>\n",
    "    dnn_neurons: !ref <dnn_neurons>\n",
    "\n",
    "emb: !new:speechbrain.nnet.embedding.Embedding\n",
    "    num_embeddings: !ref <output_neurons>\n",
    "    embedding_dim: !ref <emb_size>\n",
    "\n",
    "dec: !new:speechbrain.nnet.RNN.AttentionalRNNDecoder\n",
    "    enc_dim: !ref <dnn_neurons>\n",
    "    input_size: !ref <emb_size>\n",
    "    rnn_type: gru\n",
    "    attn_type: location\n",
    "    hidden_size: !ref <dec_neurons>\n",
    "    attn_dim: 1024\n",
    "    num_layers: 1\n",
    "    scaling: 1.0\n",
    "    channels: 10\n",
    "    kernel_size: 100\n",
    "    re_init: True\n",
    "    dropout: !ref <dropout>\n",
    "\n",
    "ctc_lin: !new:speechbrain.nnet.linear.Linear\n",
    "    input_size: !ref <dnn_neurons>\n",
    "    n_neurons: !ref <output_neurons>\n",
    "\n",
    "seq_lin: !new:speechbrain.nnet.linear.Linear\n",
    "    input_size: !ref <dec_neurons>\n",
    "    n_neurons: !ref <output_neurons>\n",
    "\n",
    "log_softmax: !new:speechbrain.nnet.activations.Softmax\n",
    "    apply_log: True\n",
    "\n",
    "lm_model: !new:speechbrain.lobes.models.RNNLM.RNNLM\n",
    "    output_neurons: !ref <output_neurons>\n",
    "    embedding_dim: !ref <emb_size>\n",
    "    activation: !name:torch.nn.LeakyReLU\n",
    "    dropout: 0.0\n",
    "    rnn_layers: 2\n",
    "    rnn_neurons: 2048\n",
    "    dnn_blocks: 1\n",
    "    dnn_neurons: 512\n",
    "    return_hidden: True  # For inference\n",
    "\n",
    "tokenizer: !new:sentencepiece.SentencePieceProcessor\n",
    "\n",
    "asr_model: !new:torch.nn.ModuleList\n",
    "    - [!ref <enc>, !ref <emb>, !ref <dec>, !ref <ctc_lin>, !ref <seq_lin>]\n",
    "\n",
    "# We compose the inference (encoder) pipeline.\n",
    "encoder: !new:speechbrain.nnet.containers.LengthsCapableSequential\n",
    "    input_shape: [null, null, !ref <n_mels>]\n",
    "    compute_features: !ref <compute_features>\n",
    "    normalize: !ref <normalize>\n",
    "    model: !ref <enc>\n",
    "\n",
    "decoder: !new:speechbrain.decoders.S2SRNNBeamSearchLM\n",
    "    embedding: !ref <emb>\n",
    "    decoder: !ref <dec>\n",
    "    linear: !ref <seq_lin>\n",
    "    language_model: !ref <lm_model>\n",
    "    bos_index: !ref <bos_index>\n",
    "    eos_index: !ref <eos_index>\n",
    "    min_decode_ratio: !ref <min_decode_ratio>\n",
    "    max_decode_ratio: !ref <max_decode_ratio>\n",
    "    beam_size: !ref <beam_size>\n",
    "    eos_threshold: !ref <eos_threshold>\n",
    "    using_max_attn_shift: !ref <using_max_attn_shift>\n",
    "    max_attn_shift: !ref <max_attn_shift>\n",
    "    coverage_penalty: !ref <coverage_penalty>\n",
    "    lm_weight: !ref <lm_weight>\n",
    "    temperature: !ref <temperature>\n",
    "    temperature_lm: !ref <temperature_lm>\n",
    "\n",
    "\n",
    "modules:\n",
    "    encoder: !ref <encoder>\n",
    "    decoder: !ref <decoder>\n",
    "    lm_model: !ref <lm_model>\n",
    "\n",
    "pretrainer: !new:speechbrain.utils.parameter_transfer.Pretrainer\n",
    "    loadables:\n",
    "        asr: !ref <asr_model>\n",
    "        lm: !ref <lm_model>\n",
    "        tokenizer: !ref <tokenizer>\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "As you can see, it is a standard YAMl file, but with a pretrainer that loads the model. It is similar to the yaml file used for training. We only have to remove all the parts that are training-specific (e.g, training parameters, optimizers, checkpointers, etc.) and add the pretrainer and `encoder`, `decoder` elements that links the needed modules with their pre-trained files. \n",
    "\n",
    "#### **Developing your own inference interface**\n",
    "\n",
    "While the `EncoderDecoderASR` class has been designed to be as generic as possible, your might require a more complex inference scheme that better fits your needs.  In this case, you have to develop your own interface. To do so, follow these steps:\n",
    "\n",
    "1. Create your custom interface inheriting from `Pretrained` (code [here](https://github.com/speechbrain/speechbrain/blob/develop/speechbrain/pretrained/interfaces.py)):\n",
    "\n",
    "\n",
    "```python\n",
    "class MySuperTask(Pretrained):\n",
    "  # Here, do not hesitate to also add some required modules\n",
    "  # for further transparency.\n",
    "  HPARAMS_NEEDED = [\"mymodule1\", \"mymodule2\"]\n",
    "  MODULES_NEEDED = [\n",
    "        \"mytask_enc\",\n",
    "        \"my_searcher\",\n",
    "  ]\n",
    "  def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Do whatever is needed here w.r.t your system\n",
    "```\n",
    "\n",
    "This will enable your class to call useful functions such as `.from_hparams()` that fetches and loads based on a HyperPyYAML file, `load_audio()` that loads a given audio file.  Likely, most of the methods that we coded in the Pretrained class will fit your need. If not, you can override them to implement your custom functionality.\n",
    "\n",
    "\n",
    "2. Develop your interface and the different functionalities. Unfortunately, we can't provide a generic enough example here. You can add **any** function to this class that you think can make inference on your data/model easier and natural. For instance, we can create here a function that simply encodes a wav file using the `mytask_enc` module.\n",
    "```python\n",
    "class MySuperTask(Pretrained):\n",
    "  # Here, do not hesitate to also add some required modules\n",
    "  # for further transparency.\n",
    "  HPARAMS_NEEDED = [\"mymodule1\", \"mymodule2\"]\n",
    "  MODULES_NEEDED = [\n",
    "        \"mytask_enc\",\n",
    "        \"my_searcher\",\n",
    "  ]\n",
    "  def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Do whatever is needed here w.r.t your system\n",
    "  \n",
    "  def encode_file(self, path):\n",
    "        waveform = self.load_audio(path)\n",
    "        # Fake a batch:\n",
    "        batch = waveform.unsqueeze(0)\n",
    "        rel_length = torch.tensor([1.0])\n",
    "        with torch.no_grad():\n",
    "          rel_lens = rel_length.to(self.device)\n",
    "          encoder_out = self.encode_batch(waveform, rel_lens)\n",
    "        \n",
    "        return encode_file\n",
    "```\n",
    "\n",
    "Now, we can use your Interface in the following way:\n",
    "```python\n",
    "from speechbrain.pretrained import MySuperTask\n",
    "\n",
    "my_model = MySuperTask.from_hparams(source=\"your_local_folder\", hparams_file='your_file.yaml', savedir=\"pretrained_model\")\n",
    "audio_file = 'your_file.wav'\n",
    "encoded = my_model.encode_file(audio_file)\n",
    "\n",
    "```\n",
    "\n",
    "As you can see, this formalism is extremely flexible and enables you to create a holistic interface that can be used to do anything you want with your pretrained model.\n",
    "\n",
    "We provide different generic interfaces for E2E ASR, speaker recognition, source separation, speech enhancement, etc. Please have a look [here](https://github.com/speechbrain/speechbrain/blob/develop/recipes/CommonVoice/ASR/seq2seq/train.py) if interested! \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z3pu0M42Pqju"
   },
   "source": [
    "## **Customize your speech recognizer**\n",
    "In a general case, you might have your own data and you would like to use your own model. Let's comment a bit more on how you can customize your recipe. \n",
    "\n",
    "**Suggestion**:  start from a recipe that is working (like the one used for this template) and only do the minimal modifications needed to customize it. Test your model step by step. Make sure your model can overfit on a tiny dataset composed of few sentences. If it doesn't overfit there is likely a bug in your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tImuOg5XP3CY"
   },
   "source": [
    "### **Train with your data**\n",
    "All you have to do when changing the dataset is to update the data preparation script such that we create the JSON files formatted as expected. The `train.py` script expects that the JSON file to be like this:\n",
    "\n",
    "\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"1867-154075-0032\": {\n",
    "    \"wav\": \"{data_root}/LibriSpeech/train-clean-5/1867/154075/1867-154075-0032.flac\",\n",
    "    \"length\": 16.09,\n",
    "    \"words\": \"AND HE BRUSHED A HAND ACROSS HIS FOREHEAD AND WAS INSTANTLY HIMSELF CALM AND COOL VERY WELL THEN IT SEEMS I'VE MADE AN ASS OF MYSELF BUT I'LL TRY TO MAKE UP FOR IT NOW WHAT ABOUT CAROLINE\"\n",
    "  },\n",
    "  \"1867-154075-0001\": {\n",
    "    \"wav\": \"{data_root}/LibriSpeech/train-clean-5/1867/154075/1867-154075-0001.flac\",\n",
    "    \"length\": 14.9,\n",
    "    \"words\": \"THAT DROPPED HIM INTO THE COAL BIN DID HE GET COAL DUST ON HIS SHOES RIGHT AND HE DIDN'T HAVE SENSE ENOUGH TO WIPE IT OFF AN AMATEUR A RANK AMATEUR I TOLD YOU SAID THE MAN OF THE SNEER WITH SATISFACTION\"\n",
    "  },\n",
    "```\n",
    "\n",
    "You have to parse your dataset and create JSON files with a unique ID for each sentence, the path of the audio signal (wav), the length of the speech sentence in seconds (length), and the word transcriptions (\"words\"). That's all!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IVCCe6cXPzJ0"
   },
   "source": [
    "### **Train with your own model**\n",
    "At some point, you might have your own model and you would like to plug it into the speech recognition pipeline. \n",
    "For instance, you might wanna replace our CRDNN encoder with something different. To do that, you have to create your own class and specify there the list of computations for your neural network. You can take a look into the models already existing in [speechbrain.lobes.models](https://github.com/speechbrain/speechbrain/tree/develop/speechbrain/lobes/models). If your model is a plain pipeline of computations, you can use the [sequential container](https://github.com/speechbrain/speechbrain/blob/develop/speechbrain/lobes/models/CRDNN.py#L14). If the model is a more complex chain of computations, you can create it as an instance of `torch.nn.Module` and define there the `__init__` and `forward` methods like [here](https://github.com/speechbrain/speechbrain/blob/develop/speechbrain/lobes/models/Xvector.py#L18).\n",
    "\n",
    "Once you defined your model, you only have to declare it in the yaml file and use it in `train.py`\n",
    "\n",
    "\n",
    "**Important:**  \n",
    "When plugging a new model, you have to tune again the most important hyperparameters of the system (e.g, learning rate, batch size, and the architectural parameters) to make the it working well.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W4pPJ0k3lJZj"
   },
   "source": [
    "\n",
    "\n",
    "## **Conclusion**\n",
    "\n",
    "In this tutorial, we showed how to create an end-to-end speech recognizer from scratch using SpeechBrain. The proposed system contains all the basic ingredients to develop a state-of-the-art system (i.e., data augmentation, tokenization, language models, beamsearch, attention, etc)\n",
    "\n",
    "We described all the steps using a small dataset only. In a real case you have to train with much more data (see for instance our [LibriSpeech recipes](https://github.com/speechbrain/speechbrain/tree/develop/recipes/LibriSpeech))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P-Trg_abjUTd"
   },
   "source": [
    "## Related Tutorials\n",
    "0. [ASRfromScratch](https://colab.research.google.com/drive/1aFgzrUv3udM_gNJNUoLaHIm78QHtxdIz?usp=sharing)\n",
    "1. [YAML hyperpatameter specification](https://colab.research.google.com/drive/1Pg9by4b6-8QD2iC0U7Ic3Vxq4GEwEdDz?usp=sharing)\n",
    "2. [Brain Class](https://colab.research.google.com/drive/1fdqTk4CTXNcrcSVFvaOKzRfLmj4fJfwa?usp=sharing)\n",
    "3. [Checkpointing](https://colab.research.google.com/drive/1VH7U0oP3CZsUNtChJT2ewbV_q1QX8xre?usp=sharing)\n",
    "4. [Data-io](https://colab.research.google.com/drive/1AiVJZhZKwEI4nFGANKXEe-ffZFfvXKwH?usp=sharing)\n",
    "5. [Tokenizer](https://colab.research.google.com/drive/12yE3myHSH-eUxzNM0-FLtEOhzdQoLYWe?usp=sharing)\n",
    "6. [Speech Features](https://colab.research.google.com/drive/1CI72Xyay80mmmagfLaIIeRoDgswWHT_g?usp=sharing)\n",
    "7. [Speech Augmentation](https://colab.research.google.com/drive/1JJc4tBhHNXRSDM2xbQ3Z0jdDQUw4S5lr?usp=sharing)\n",
    "8. [Environmental Corruption](https://colab.research.google.com/drive/1mAimqZndq0BwQj63VcDTr6_uCMC6i6Un?usp=sharing)\n",
    "9. [MultiGPU Training](https://colab.research.google.com/drive/13pBUacPiotw1IvyffvGZ-HrtBr9T6l15?usp=sharing)\n",
    "10. [Pretrain and Fine-tune](https://colab.research.google.com/drive/1LN7R3U3xneDgDRK2gC5MzGkLysCWxuC3?usp=sharing)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of ASRfromScratch.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "03842889ca4f42ddabecf025fa71cfd1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "07aadeb0ed4c4fc7a9ea91b3938a2e02": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1060fe2ec0774468a5903acb2b7f9a87": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1247b9b12f884d00b318569af5d1c33f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "16fd875c6034418d844de3aa5f18d8d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2569d3526a6742828af2d9360a67d312",
       "IPY_MODEL_472f38e4f2514fcca5840a19bd6b80c7",
       "IPY_MODEL_cf48aa6b98014f97a974ca398287b6a4"
      ],
      "layout": "IPY_MODEL_1247b9b12f884d00b318569af5d1c33f"
     }
    },
    "19800374b3cc44db9c82637d3981265a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1b9fd09c88184811ad52d2b03c60422d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1d871e41a1c346c0941ff8bf286a8fc2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "209d0ef110b343d08759c8daeaeb78b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "20d770de4aa74489a9f816cf038809b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e3fecdd0618745928f6d778aebc05afa",
      "max": 104390,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d7887ea69bfe4ffa931d1a730d0e9e02",
      "value": 104390
     }
    },
    "2569d3526a6742828af2d9360a67d312": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_afa3cd14810244bf92d8814bd695f3b6",
      "placeholder": "​",
      "style": "IPY_MODEL_07aadeb0ed4c4fc7a9ea91b3938a2e02",
      "value": "Downloading: 100%"
     }
    },
    "26573bd508184e5faac952104b8784b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2a8a07ff265941c78d3dd4915f5c11a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_19800374b3cc44db9c82637d3981265a",
      "placeholder": "​",
      "style": "IPY_MODEL_209d0ef110b343d08759c8daeaeb78b8",
      "value": " 480M/480M [00:08&lt;00:00, 55.6MB/s]"
     }
    },
    "2b38891bfbf148c99c8f770a46c129dd": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2b7c662594354cfa9327ba21f93fe51e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3af10f6cc57a47719bfca37ee59eb84f",
       "IPY_MODEL_b176842c64324374b0d42325dbdae328",
       "IPY_MODEL_55e848a9b4264094b5e9fa9e29adf605"
      ],
      "layout": "IPY_MODEL_b5ad90b6a6924ae3993ff2a9751c917c"
     }
    },
    "2e7f8ea4c64d49478573974925975f18": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2e96792c1c3949ef9deb839df5c3f8e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_418580459dc54712a947fbdf6ec8bd37",
      "placeholder": "​",
      "style": "IPY_MODEL_f207a275c2624be9b89d74b2f0fe2738",
      "value": "Downloading: 100%"
     }
    },
    "31a565eb19ab4091a965a1191a0f9e65": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "326aecdc3b3342ceb3d0bb82c1b6ca93": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "32f78deb548546a788a92d9507f6eb72": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "34181d8ec5fa45fe858b56c3eb86aed6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3f9828cfafc14ee9935039fd43f4c6a0",
      "placeholder": "​",
      "style": "IPY_MODEL_5ccec8b8ad064037bf7f55ebb9e23907",
      "value": " 104k/104k [00:00&lt;00:00, 297kB/s]"
     }
    },
    "3af10f6cc57a47719bfca37ee59eb84f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_855230c0c2fd46fcb28a63065ce5ebd7",
      "placeholder": "​",
      "style": "IPY_MODEL_326aecdc3b3342ceb3d0bb82c1b6ca93",
      "value": "Downloading: 100%"
     }
    },
    "3f9828cfafc14ee9935039fd43f4c6a0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "418580459dc54712a947fbdf6ec8bd37": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4377bcef40f142efaa448b925d36730f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "472f38e4f2514fcca5840a19bd6b80c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cb36b99b7ee24cefa46590ecc0122b72",
      "max": 212420087,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_69bcb272473c45caac52ac93725ab4e1",
      "value": 212420087
     }
    },
    "48901abf068b4242a014050db20b0631": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "55e848a9b4264094b5e9fa9e29adf605": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_03842889ca4f42ddabecf025fa71cfd1",
      "placeholder": "​",
      "style": "IPY_MODEL_c5cd1cec1745490aa00041eaccf3f4e3",
      "value": " 253k/253k [00:00&lt;00:00, 3.82MB/s]"
     }
    },
    "5a908cae48844f8ca14baf189d7e58bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1060fe2ec0774468a5903acb2b7f9a87",
      "placeholder": "​",
      "style": "IPY_MODEL_772373f931bc42869027b4db253737ad",
      "value": " 4.42k/4.42k [00:00&lt;00:00, 80.0kB/s]"
     }
    },
    "5ccec8b8ad064037bf7f55ebb9e23907": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6248b94de0464c2b933add8e4f6b7af4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_eff25baa67674ee58e9715e78cd5b7a1",
       "IPY_MODEL_f917b17d56734fb1b8cb471e51b15328",
       "IPY_MODEL_2a8a07ff265941c78d3dd4915f5c11a1"
      ],
      "layout": "IPY_MODEL_48901abf068b4242a014050db20b0631"
     }
    },
    "676c443474a64391bb0398cfd3b7d7c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "69bcb272473c45caac52ac93725ab4e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "742aaf25ce0b4911ad1b17aced2a411f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2b38891bfbf148c99c8f770a46c129dd",
      "max": 4420,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1d871e41a1c346c0941ff8bf286a8fc2",
      "value": 4420
     }
    },
    "772373f931bc42869027b4db253737ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "793ef14b7b004096981c55e7bf4fbdae": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ba5f07c308fa42f0963848266d7e376d",
       "IPY_MODEL_742aaf25ce0b4911ad1b17aced2a411f",
       "IPY_MODEL_5a908cae48844f8ca14baf189d7e58bd"
      ],
      "layout": "IPY_MODEL_c8edc1eae8084ade9b3e02be21ac6b36"
     }
    },
    "81573628e9ef4ec18a325a0d62b790e8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "855230c0c2fd46fcb28a63065ce5ebd7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8d8f39950d0d416395d3c1c2afc03396": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9e8b3e216161422bb18c494ee3da756d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2e96792c1c3949ef9deb839df5c3f8e9",
       "IPY_MODEL_20d770de4aa74489a9f816cf038809b4",
       "IPY_MODEL_34181d8ec5fa45fe858b56c3eb86aed6"
      ],
      "layout": "IPY_MODEL_4377bcef40f142efaa448b925d36730f"
     }
    },
    "afa3cd14810244bf92d8814bd695f3b6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b176842c64324374b0d42325dbdae328": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_81573628e9ef4ec18a325a0d62b790e8",
      "max": 253217,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cc771ea227a94bcf9ffda70f5085f90f",
      "value": 253217
     }
    },
    "b5ad90b6a6924ae3993ff2a9751c917c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ba5f07c308fa42f0963848266d7e376d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8d8f39950d0d416395d3c1c2afc03396",
      "placeholder": "​",
      "style": "IPY_MODEL_676c443474a64391bb0398cfd3b7d7c5",
      "value": "Downloading: 100%"
     }
    },
    "c5cd1cec1745490aa00041eaccf3f4e3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c8edc1eae8084ade9b3e02be21ac6b36": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cb36b99b7ee24cefa46590ecc0122b72": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cc771ea227a94bcf9ffda70f5085f90f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cf48aa6b98014f97a974ca398287b6a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2e7f8ea4c64d49478573974925975f18",
      "placeholder": "​",
      "style": "IPY_MODEL_26573bd508184e5faac952104b8784b5",
      "value": " 212M/212M [00:04&lt;00:00, 51.5MB/s]"
     }
    },
    "d7887ea69bfe4ffa931d1a730d0e9e02": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e239d2cbb12346ffb706cacab4dca38a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e3fecdd0618745928f6d778aebc05afa": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eff25baa67674ee58e9715e78cd5b7a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1b9fd09c88184811ad52d2b03c60422d",
      "placeholder": "​",
      "style": "IPY_MODEL_31a565eb19ab4091a965a1191a0f9e65",
      "value": "Downloading: 100%"
     }
    },
    "f207a275c2624be9b89d74b2f0fe2738": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f917b17d56734fb1b8cb471e51b15328": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e239d2cbb12346ffb706cacab4dca38a",
      "max": 479555971,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_32f78deb548546a788a92d9507f6eb72",
      "value": 479555971
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
